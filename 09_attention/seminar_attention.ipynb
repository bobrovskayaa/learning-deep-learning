{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_attention_right.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xNB4KG1SPdtr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Traslation with Attention\n",
        "Disclaimer: This notebook is an adopted version of [this repository](https://github.com/keon/seq2seq) and [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
        "\n",
        "In this tutorial we'll learn how to build recurrent neural network with attention mechanism to automatically translate from German to English! \n",
        "\n",
        "<img src=\"static/model.png\" width=300 align=\"center\"/>"
      ]
    },
    {
      "metadata": {
        "id": "1NZ6drkdPdtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ]
    },
    {
      "metadata": {
        "id": "DHw81BzZPdt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "\n",
        "# for text processing\n",
        "import spacy\n",
        "import torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHK7tlC7PduD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Parameters:"
      ]
    },
    {
      "metadata": {
        "id": "N6o3GYOgPduG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "batch_size = 32\n",
        "hidden_size = 512\n",
        "embed_size = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSu_UXL4PduQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "1. To train NMT model in supervised manner we need dataset of parallel texts for 2 (or more) languages. Today we'll use **Multi30k** dataset from `torchtext` package. It's a small dataset containing exactly what we need - parallel sentences in German and English."
      ]
    },
    {
      "metadata": {
        "id": "vkG2882qPduU",
        "colab_type": "code",
        "outputId": "e14a12ee-38ca-4aa3-ee38-ca365916bc76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "torchtext.datasets.Multi30k"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.datasets.translation.Multi30k"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "MwK8l6CPPduh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. To tokenize text we'll use `spacy`. Before using it you have to download German and English language packages:\n",
        "```bash\n",
        "python3 -m spacy download de\n",
        "python3 -m spacy download en\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "oYrUEu5iQQSu",
        "colab_type": "code",
        "outputId": "468eae87-abbb-4b66-8ea8-cd30e29c8968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "! python3 -m spacy download de"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AtZu--YDQSAx",
        "colab_type": "code",
        "outputId": "fae35bcb-2603-4884-8ac2-96e4fb72f57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "! python3 -m spacy download en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qsFuzE2SPduk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. To operate with text we'll use handy `torchtext` abstractions: [Field](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field) and [BucketIterator](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator). Read the docs on these classes for more details."
      ]
    },
    {
      "metadata": {
        "id": "LDyyzFZbPdun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's combine all data loading and preparation stuff in one function:"
      ]
    },
    {
      "metadata": {
        "id": "sfx9XJ1qPduq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(batch_size):\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "    url = re.compile('(<url>.*</url>)')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
        "\n",
        "    DE = torchtext.data.Field(tokenize=tokenize_de, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
        "    EN = torchtext.data.Field(tokenize=tokenize_en, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
        "    \n",
        "    train, val, test = torchtext.datasets.Multi30k.splits(exts=('.de', '.en'), fields=(DE, EN))\n",
        "    DE.build_vocab(train.src, min_freq=2)\n",
        "    EN.build_vocab(train.trg, max_size=10000)\n",
        "    train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
        "            (train, val, test), batch_size=batch_size, repeat=False)\n",
        "    return train_iter, val_iter, test_iter, DE, EN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_O9oDPsZPdu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter, DE, EN = load_dataset(batch_size)\n",
        "de_size, en_size = len(DE.vocab), len(EN.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLXrwaXEPdvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Grab one batch:"
      ]
    },
    {
      "metadata": {
        "id": "LghoO6AtPdvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LckK4HzbPdvX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`batch` has 2 attributes: `src` and `trg`. Each contains tuple with numerical representation of the sentences with similar lengths (padded) and their original lengths:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "rVn8JfoGPdvc",
        "colab_type": "code",
        "outputId": "f0fe2fde-ea4b-4ee7-cafd-c430ddfca37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"input batch shape\", batch.src[0].shape)\n",
        "print(\"input batch lengths\", batch.src[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input batch shape torch.Size([24, 32])\n",
            "input batch lengths tensor([ 9, 13, 12, 22, 12, 13, 14, 21, 15,  9, 14, 11, 12, 24, 11, 11,  9, 16,\n",
            "         7, 19, 16, 17, 12, 12, 20, 14,  7, 11, 21, 23, 13, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yzRppqyiPdvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using `DE` and `EN` you can convert from string representation to numerical and back (`.stoi` and `.itos` methods):"
      ]
    },
    {
      "metadata": {
        "id": "4HSu_hIGPdvu",
        "colab_type": "code",
        "outputId": "6d8a2e18-4529-4f13-fdd3-94dc63e8ddab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"encoded sentence\", batch.src[0][:, 0])\n",
        "print(\"encoded sentence\", [DE.vocab.itos[token] for token in batch.src[0][:, 0]])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded sentence tensor([   2,   67,    7, 2589,   11,    6,  300,    4,    3,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
            "encoded sentence ['<sos>', 'Kinder', 'in', 'Kanus', 'auf', 'einem', 'Fluss', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "amijHdqsPdv5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now have data, it's time build models!"
      ]
    },
    {
      "metadata": {
        "id": "aegQ6QSGPdv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 1 (2 points). Seq2Seq\n",
        "A **Sequence to Sequence** network, or **seq2seq** network, or **Encoder-Decoder** network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence. Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the “meaning” of the input sequence into a single vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "<img src=\"static/seq2seq.png\" width=1000 align=\"center\"/>"
      ]
    },
    {
      "metadata": {
        "id": "9WBVnFx9PdwA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "The encoder of a seq2seq network is a bidirectional GRU RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word. To keep hidden size of fixed shape we sum outputs over 2 directions.\n",
        "\n",
        "**Note:** here we use [nn.GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU), not [nn.GRUCell](https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell). Read the docs to understand the differences."
      ]
    },
    {
      "metadata": {
        "id": "FwludjNKPdwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size,\n",
        "                 n_layers=1, dropout=0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.embed = nn.Embedding(input_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
        "\n",
        "    def forward(self, src, hidden=None):\n",
        "        \"\"\"Encodes input sequence\n",
        "\n",
        "        Args:\n",
        "            src (torch tensor of shape (t, b)): input sequence\n",
        "            hidden (torch tensor of shape (n_layers * n_directions, b, h)): prev hidden state (can be None)\n",
        "        \n",
        "        Returns:\n",
        "            outputs (torch tensor of shape (t, b, h)): encoded sequence (dicrections are summed)\n",
        "            hidden (torch tensor of shape (n_layers * n_directions, b, h)): hidden state\n",
        "        \"\"\"\n",
        "        embedded = self.embed(src)\n",
        "        outputs, hidden = self.gru(embedded, hidden)\n",
        "        \n",
        "        # sum bidirectional outputs\n",
        "        outputs = outputs.view(outputs.shape[0], outputs.shape[1], 2, self.hidden_size)\n",
        "        outputs = outputs.sum(dim=2)\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POjvMqcVPdwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "In the simplest seq2seq decoder uses only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder. At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
        "    \n",
        "But in this tutorial we'll pump our decoder with attention mechanism!"
      ]
    },
    {
      "metadata": {
        "id": "0wehQH73PdwM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder with attention"
      ]
    },
    {
      "metadata": {
        "id": "T5NQfRjePdwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
        "\n",
        "<img src=\"static/attention.png\" width=500 align=\"center\"/>\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward layer, using the decoder’s input and hidden state as inputs. Below you can find short description of the so-called Bahdanau attention mechanism (details can be found in the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)).\n",
        "\n",
        "Attention weights ($\\alpha_{ij}$):\n",
        "\n",
        "$$\n",
        "    e_{ij} = f(s_{i-1}, h_j) = v \\tanh(W [s_{i-1}, h_j] + b) \\\\\n",
        "    \\alpha_{ij} = softmax(e_{ij}) = \\frac{\\exp{e_{ij}}}{\\sum_j{\\exp{e_{ij}}}}\n",
        "$$\n",
        "\n",
        "Here $s_{i-1}$ - hidden state of decoder (`hidden`), $h_j$ - hidden state of encoder (`encoder_outputs`), $v$, $W$, $b$ - learnable parameters.\n",
        "\n",
        "Let's implement it:"
      ]
    },
    {
      "metadata": {
        "id": "Th4MrIQqPdwR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "        \n",
        "        # setup attention parameters\n",
        "        self.v = nn.Parameter(torch.zeros(hidden_size))\n",
        "        \n",
        "        stdv = 1. / np.sqrt(self.v.shape[0])\n",
        "        self.v.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \"\"\"Calculates attention weights\n",
        "\n",
        "        Args:\n",
        "            hidden (torch tensor of shape (b, h)): prev hidden state (can be None)\n",
        "            encoder_outputs (torch tensor of shape (t, b, h)): encoded sequence\n",
        "        \n",
        "        Returns:\n",
        "            attn_weights (torch tensor of shape (b, 1, t)): attention weights\n",
        "        \"\"\" \n",
        "        \n",
        "        timestep = encoder_outputs.shape[0]\n",
        "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)  # [B*T*H]\n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
        "        \n",
        "        # [B*T*2H]->[B*T*H]\n",
        "        ## concat h and encoder_outputs, feed to self.attn and then to softmax \n",
        "        energy = energy = F.softmax(self.attn(torch.cat([h, encoder_outputs], dim=2)), dim=1)\n",
        "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
        "        \n",
        "        v = self.v.repeat(encoder_outputs.shape[0], 1).unsqueeze(1)  # [B*1*H]\n",
        "        ## multiply by v vector to get shape [B*1*T]\n",
        "        attn_weights = torch.bmm(v, energy)\n",
        "        \n",
        "        return attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dp6jykS4Pdwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's insert attention mechanism to decoder:"
      ]
    },
    {
      "metadata": {
        "id": "RamLXFyCPdwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, output_size,\n",
        "                 n_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embed = nn.Embedding(output_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
        "        self.attention = Attention(hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input, last_hidden, encoder_outputs):\n",
        "        \"\"\"Decodes with attention token by token\n",
        "\n",
        "        Args:\n",
        "            input (torch tensor of shape (b,)): input token\n",
        "            last_hidden (torch tensor of shape (1, b, h)): last hidden\n",
        "            encoder_outputs (torch tensor of shape (t, b, h)): encoded sequence\n",
        "        \n",
        "        Returns:\n",
        "            output (torch tensor of shape (b, vocab_size)): ouput token distribution\n",
        "            hidden (torch tensor of shape (1, b, h)): hidden state\n",
        "            attn_weights (torch tensor of shape (b, 1, t)): attention weights\n",
        "        \"\"\"\n",
        "        # get the embedding of the current input word (last output word)\n",
        "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # calculate attention weights and apply to encoder outputs\n",
        "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)  # (B,1,T)\n",
        "        ## apply attention weights to encoder_outputs to get shape # (B,1,N) (don't forget to transpose encoder_outputs)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1))\n",
        "        context = context.transpose(0, 1)  # (1,B,N)\n",
        "        \n",
        "        # combine embedded input word and attended context, run through RNN\n",
        "        rnn_input = torch.cat([embedded, context], 2)\n",
        "        ## forward recurrent unit \n",
        "        output, hidden = self.gru(rnn_input, last_hidden)\n",
        "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
        "        \n",
        "        context = context.squeeze(0)\n",
        "        output = self.out(torch.cat([output, context], 1))\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a749MZTzPdwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrap in a single Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "KacIKZilPdwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, sos_token, max_len):\n",
        "        \"\"\"Sequence-to-sequence inference\n",
        "\n",
        "        Args:\n",
        "            src (torch tensor of shape (t, b)): input sequence\n",
        "        \n",
        "        Returns:\n",
        "            outputs (torch tensor of shape (b, vocab_size)): ouput token distribution\n",
        "        \"\"\"\n",
        "        device = src.device\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        vocab_size = self.decoder.output_size\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(device)\n",
        "\n",
        "        encoder_output, hidden = self.encoder(src)\n",
        "        hidden = hidden[:self.decoder.n_layers]\n",
        "        \n",
        "        output = torch.full((batch_size,), sos_token, dtype=torch.long).to(device)\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            ## apply decoder\n",
        "            output, hidden, attn_weights = self.decoder(output, hidden, encoder_output)\n",
        "            outputs[t] = output\n",
        "            \n",
        "            top1 = output.data.max(1)[1]\n",
        "            output = top1.to(device)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uhXNUBuPdwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2 (1 point). Train-loop"
      ]
    },
    {
      "metadata": {
        "id": "driMgZTePdwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Parameters:"
      ]
    },
    {
      "metadata": {
        "id": "fKO2XGLMPdwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 0.0001\n",
        "grad_clip = 10.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frfNoIUQPdw8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model:"
      ]
    },
    {
      "metadata": {
        "id": "GawWNBuBPdxE",
        "colab_type": "code",
        "outputId": "4a71dc2f-1793-45a5-9d62-3f4642aac998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(de_size, embed_size, hidden_size, n_layers=2, dropout=0.5)\n",
        "decoder = Decoder(embed_size, hidden_size, en_size, n_layers=1, dropout=0.0)\n",
        "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr=lr)\n",
        "print(seq2seq)\n",
        "\n",
        "trg_sos_token = EN.vocab.stoi['<sos>']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(8011, 256)\n",
            "    (gru): GRU(256, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embed): Embedding(10004, 256)\n",
            "    (dropout): Dropout(p=0.0, inplace)\n",
            "    (attention): Attention(\n",
            "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    )\n",
            "    (gru): GRU(768, 512)\n",
            "    (out): Linear(in_features=1024, out_features=10004, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9enj_3_QPdxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wrapped `train` and `evaluate` ops:"
      ]
    },
    {
      "metadata": {
        "id": "MKqbohtOPdxd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter, vocab_size, device, DE, EN):\n",
        "    model.eval()\n",
        "    pad = EN.vocab.stoi['<pad>']\n",
        "    total_loss = 0\n",
        "    for b, batch in enumerate(val_iter):\n",
        "        src, len_src = batch.src\n",
        "        trg, len_trg = batch.trg\n",
        "\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # apply model\n",
        "            output = seq2seq(src, trg_sos_token, max_len=trg.shape[0])\n",
        "            \n",
        "            # calculate nll loss\n",
        "            # 1) don't take into account first token (it's always <sos>)\n",
        "            # 2) don't take into account pad token (ignore_index argument)\n",
        "            loss = F.nll_loss(output[1:].view(-1, vocab_size), trg[1:].view(-1), ignore_index=pad)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_iter)\n",
        "\n",
        "\n",
        "def train(e, model, optimizer, train_iter, vocab_size, device, grad_clip, DE, EN):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pad = EN.vocab.stoi['<pad>']\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        src, len_src = batch.src\n",
        "        trg, len_trg = batch.trg\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # apply model\n",
        "        output = seq2seq(src, trg_sos_token, max_len=trg.shape[0])\n",
        "        \n",
        "        # calculate nll loss\n",
        "        # 1) don't take into account first token (it's always <sos>)\n",
        "        # 2) don't take into account pad token (ignore_index argument)\n",
        "        loss = F.nll_loss(output[1:].view(-1, vocab_size), trg[1:].view(-1), ignore_index=pad)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # clip gradients using nn.utils.clip_grad_norm_ by `grad_clip` value\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if b % 10 == 0 and b != 0:\n",
        "            total_loss = total_loss / 10\n",
        "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" % (b, total_loss, np.exp(total_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnga_101Pdxk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To monitor how our training is going on, let's translate fixed batch of sentences every epoch:"
      ]
    },
    {
      "metadata": {
        "id": "5Ci-RL4JPdxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fixed_test_batch = next(iter(test_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okk5HbONPdxt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_translations(seq2seq, batch, device, trg_sos_token, max_len=10, max_examples=5):\n",
        "    sentence_encoded = batch.src[0].to(device)\n",
        "    \n",
        "    for example_i in range(min(batch.src[0].shape[1], max_examples)):\n",
        "        input_encoded = batch.src[0][:, example_i]\n",
        "        input = \" \".join([DE.vocab.itos[index] for index in input_encoded][1:batch.src[1][example_i]])\n",
        "        \n",
        "        result_encoded = seq2seq(sentence_encoded, trg_sos_token, max_len)\n",
        "        result_encoded = result_encoded.argmax(dim=2)[:, example_i]\n",
        "        pred = \" \".join([EN.vocab.itos[index] for index in result_encoded][1:])\n",
        "\n",
        "        gt_encoded = batch.trg[0][:, example_i]\n",
        "        gt = \" \".join([EN.vocab.itos[index] for index in gt_encoded][1:batch.trg[1][example_i]])\n",
        "        \n",
        "        print(\"input:\\t\", input)\n",
        "        print(\"pred:\\t\", pred)\n",
        "        print(\"gt:\\t\", gt)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUkXQhlcPdx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run train-loop:"
      ]
    },
    {
      "metadata": {
        "id": "8Nx1B8KFPdx4",
        "colab_type": "code",
        "outputId": "1af542e5-f4a8-4712-9f20-5a91b6381d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19760
        }
      },
      "cell_type": "code",
      "source": [
        "best_val_loss = None\n",
        "\n",
        "for e in range(1, 11):\n",
        "    train(e, seq2seq, optimizer, train_iter, en_size, device, grad_clip, DE, EN)\n",
        "    val_loss = evaluate(seq2seq, val_iter, en_size, device, DE, EN)\n",
        "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, np.exp(val_loss)))\n",
        "\n",
        "    # save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        print(\"[!] saving model...\")\n",
        "        if not os.path.isdir(\"weights\"):\n",
        "            os.makedirs(\"weights\")\n",
        "        torch.save(seq2seq.state_dict(), 'weights/seq2seq_%d.pth' % (e))\n",
        "        best_val_loss = val_loss\n",
        "    \n",
        "    print(\"Samples from test:\")\n",
        "    show_translations(seq2seq, fixed_test_batch, device, trg_sos_token, max_len=20, max_examples=5)\n",
        "    print()\n",
        "        \n",
        "test_loss = evaluate(seq2seq, test_iter, en_size, device, DE, EN)\n",
        "print(\"[TEST] loss:%5.2f\" % test_loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10][loss: 9.59][pp:14614.16]\n",
            "[20][loss: 6.87][pp:962.38]\n",
            "[30][loss: 6.01][pp:406.08]\n",
            "[40][loss: 5.73][pp:307.92]\n",
            "[50][loss: 5.54][pp:255.86]\n",
            "[60][loss: 5.41][pp:223.23]\n",
            "[70][loss: 5.38][pp:216.36]\n",
            "[80][loss: 5.32][pp:205.30]\n",
            "[90][loss: 5.33][pp:206.43]\n",
            "[100][loss: 5.37][pp:214.50]\n",
            "[110][loss: 5.27][pp:193.46]\n",
            "[120][loss: 5.22][pp:184.52]\n",
            "[130][loss: 5.27][pp:193.74]\n",
            "[140][loss: 5.22][pp:184.28]\n",
            "[150][loss: 5.26][pp:192.25]\n",
            "[160][loss: 5.17][pp:175.74]\n",
            "[170][loss: 5.15][pp:172.75]\n",
            "[180][loss: 5.25][pp:191.20]\n",
            "[190][loss: 5.25][pp:190.83]\n",
            "[200][loss: 5.07][pp:159.28]\n",
            "[210][loss: 5.16][pp:174.52]\n",
            "[220][loss: 5.09][pp:162.63]\n",
            "[230][loss: 5.10][pp:163.93]\n",
            "[240][loss: 5.10][pp:164.23]\n",
            "[250][loss: 5.15][pp:172.66]\n",
            "[260][loss: 5.13][pp:168.42]\n",
            "[270][loss: 5.05][pp:155.25]\n",
            "[280][loss: 5.07][pp:159.32]\n",
            "[290][loss: 5.08][pp:160.33]\n",
            "[300][loss: 5.03][pp:153.52]\n",
            "[310][loss: 4.96][pp:142.49]\n",
            "[320][loss: 4.98][pp:145.34]\n",
            "[330][loss: 5.09][pp:161.63]\n",
            "[340][loss: 4.98][pp:145.21]\n",
            "[350][loss: 5.03][pp:152.31]\n",
            "[360][loss: 4.96][pp:142.62]\n",
            "[370][loss: 4.94][pp:140.00]\n",
            "[380][loss: 4.88][pp:131.47]\n",
            "[390][loss: 4.93][pp:139.01]\n",
            "[400][loss: 4.90][pp:134.19]\n",
            "[410][loss: 4.84][pp:126.70]\n",
            "[420][loss: 4.90][pp:134.20]\n",
            "[430][loss: 5.00][pp:148.54]\n",
            "[440][loss: 4.70][pp:109.58]\n",
            "[450][loss: 4.88][pp:131.26]\n",
            "[460][loss: 4.73][pp:113.14]\n",
            "[470][loss: 4.76][pp:116.47]\n",
            "[480][loss: 4.72][pp:112.49]\n",
            "[490][loss: 4.82][pp:124.03]\n",
            "[500][loss: 4.75][pp:115.78]\n",
            "[510][loss: 4.74][pp:113.96]\n",
            "[520][loss: 4.73][pp:112.91]\n",
            "[530][loss: 4.76][pp:116.56]\n",
            "[540][loss: 4.66][pp:106.06]\n",
            "[550][loss: 4.64][pp:103.79]\n",
            "[560][loss: 4.70][pp:109.90]\n",
            "[570][loss: 4.62][pp:101.15]\n",
            "[580][loss: 4.62][pp:101.22]\n",
            "[590][loss: 4.63][pp:102.68]\n",
            "[600][loss: 4.70][pp:109.40]\n",
            "[610][loss: 4.57][pp:96.45]\n",
            "[620][loss: 4.65][pp:104.49]\n",
            "[630][loss: 4.61][pp:100.80]\n",
            "[640][loss: 4.60][pp:99.76]\n",
            "[650][loss: 4.63][pp:102.98]\n",
            "[660][loss: 4.54][pp:93.76]\n",
            "[670][loss: 4.49][pp:88.70]\n",
            "[680][loss: 4.57][pp:96.22]\n",
            "[690][loss: 4.44][pp:84.49]\n",
            "[700][loss: 4.59][pp:98.73]\n",
            "[710][loss: 4.57][pp:96.44]\n",
            "[720][loss: 4.46][pp:86.27]\n",
            "[730][loss: 4.37][pp:79.08]\n",
            "[740][loss: 4.50][pp:89.66]\n",
            "[750][loss: 4.49][pp:88.75]\n",
            "[760][loss: 4.39][pp:80.93]\n",
            "[770][loss: 4.51][pp:90.84]\n",
            "[780][loss: 4.46][pp:86.41]\n",
            "[790][loss: 4.33][pp:75.57]\n",
            "[800][loss: 4.44][pp:84.78]\n",
            "[810][loss: 4.48][pp:88.38]\n",
            "[820][loss: 4.35][pp:77.11]\n",
            "[830][loss: 4.37][pp:78.78]\n",
            "[840][loss: 4.32][pp:75.23]\n",
            "[850][loss: 4.45][pp:85.79]\n",
            "[860][loss: 4.35][pp:77.84]\n",
            "[870][loss: 4.28][pp:71.89]\n",
            "[880][loss: 4.24][pp:69.71]\n",
            "[890][loss: 4.41][pp:81.95]\n",
            "[900][loss: 4.39][pp:81.00]\n",
            "[Epoch:1] val_loss:4.229 | val_pp:68.62S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t The are is to to to to . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing playing playing playing playing . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two dogs are on the the the . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A dog dog dog dog dog . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 4.63][pp:102.34]\n",
            "[20][loss: 4.19][pp:65.74]\n",
            "[30][loss: 4.14][pp:63.12]\n",
            "[40][loss: 4.09][pp:59.76]\n",
            "[50][loss: 4.19][pp:66.18]\n",
            "[60][loss: 4.15][pp:63.31]\n",
            "[70][loss: 4.07][pp:58.76]\n",
            "[80][loss: 4.17][pp:64.70]\n",
            "[90][loss: 4.14][pp:62.69]\n",
            "[100][loss: 4.03][pp:56.52]\n",
            "[110][loss: 4.12][pp:61.34]\n",
            "[120][loss: 4.11][pp:60.67]\n",
            "[130][loss: 4.13][pp:62.23]\n",
            "[140][loss: 4.06][pp:58.24]\n",
            "[150][loss: 4.10][pp:60.09]\n",
            "[160][loss: 4.24][pp:69.08]\n",
            "[170][loss: 4.14][pp:62.57]\n",
            "[180][loss: 4.06][pp:57.97]\n",
            "[190][loss: 4.08][pp:59.01]\n",
            "[200][loss: 4.09][pp:59.71]\n",
            "[210][loss: 4.05][pp:57.43]\n",
            "[220][loss: 4.06][pp:58.23]\n",
            "[230][loss: 3.97][pp:52.97]\n",
            "[240][loss: 4.03][pp:56.19]\n",
            "[250][loss: 4.03][pp:56.22]\n",
            "[260][loss: 4.09][pp:59.46]\n",
            "[270][loss: 4.12][pp:61.32]\n",
            "[280][loss: 4.12][pp:61.39]\n",
            "[290][loss: 4.15][pp:63.34]\n",
            "[300][loss: 4.01][pp:55.33]\n",
            "[310][loss: 4.05][pp:57.14]\n",
            "[320][loss: 4.08][pp:58.87]\n",
            "[330][loss: 4.09][pp:59.88]\n",
            "[340][loss: 4.03][pp:56.23]\n",
            "[350][loss: 4.05][pp:57.46]\n",
            "[360][loss: 3.99][pp:54.26]\n",
            "[370][loss: 3.96][pp:52.71]\n",
            "[380][loss: 4.07][pp:58.71]\n",
            "[390][loss: 4.02][pp:55.72]\n",
            "[400][loss: 3.98][pp:53.43]\n",
            "[410][loss: 4.01][pp:55.28]\n",
            "[420][loss: 4.05][pp:57.14]\n",
            "[430][loss: 4.06][pp:57.70]\n",
            "[440][loss: 3.96][pp:52.70]\n",
            "[450][loss: 4.03][pp:56.34]\n",
            "[460][loss: 4.04][pp:56.79]\n",
            "[470][loss: 4.02][pp:55.80]\n",
            "[480][loss: 3.96][pp:52.28]\n",
            "[490][loss: 4.00][pp:54.75]\n",
            "[500][loss: 3.86][pp:47.58]\n",
            "[510][loss: 3.82][pp:45.82]\n",
            "[520][loss: 3.85][pp:47.11]\n",
            "[530][loss: 3.94][pp:51.66]\n",
            "[540][loss: 3.95][pp:51.82]\n",
            "[550][loss: 4.00][pp:54.51]\n",
            "[560][loss: 3.96][pp:52.50]\n",
            "[570][loss: 3.93][pp:50.78]\n",
            "[580][loss: 3.97][pp:52.97]\n",
            "[590][loss: 3.97][pp:52.78]\n",
            "[600][loss: 3.96][pp:52.66]\n",
            "[610][loss: 3.94][pp:51.66]\n",
            "[620][loss: 3.91][pp:49.98]\n",
            "[630][loss: 3.94][pp:51.40]\n",
            "[640][loss: 3.65][pp:38.49]\n",
            "[650][loss: 3.95][pp:51.76]\n",
            "[660][loss: 3.90][pp:49.23]\n",
            "[670][loss: 3.93][pp:51.07]\n",
            "[680][loss: 3.99][pp:54.32]\n",
            "[690][loss: 3.88][pp:48.49]\n",
            "[700][loss: 3.95][pp:51.86]\n",
            "[710][loss: 3.80][pp:44.68]\n",
            "[720][loss: 3.88][pp:48.36]\n",
            "[730][loss: 3.94][pp:51.37]\n",
            "[740][loss: 3.77][pp:43.41]\n",
            "[750][loss: 3.85][pp:46.89]\n",
            "[760][loss: 3.74][pp:42.17]\n",
            "[770][loss: 3.97][pp:53.00]\n",
            "[780][loss: 3.89][pp:48.67]\n",
            "[790][loss: 3.84][pp:46.67]\n",
            "[800][loss: 3.84][pp:46.72]\n",
            "[810][loss: 3.82][pp:45.43]\n",
            "[820][loss: 3.82][pp:45.76]\n",
            "[830][loss: 3.85][pp:46.91]\n",
            "[840][loss: 3.91][pp:50.04]\n",
            "[850][loss: 3.89][pp:48.77]\n",
            "[860][loss: 3.85][pp:47.01]\n",
            "[870][loss: 3.82][pp:45.67]\n",
            "[880][loss: 3.83][pp:45.99]\n",
            "[890][loss: 3.84][pp:46.36]\n",
            "[900][loss: 3.81][pp:44.93]\n",
            "[Epoch:2] val_loss:3.768 | val_pp:43.28S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Construction workers workers the the to the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys boys soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players are field field field field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A dog dog dog walking dog . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People people are a a . . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 4.01][pp:55.37]\n",
            "[20][loss: 3.51][pp:33.52]\n",
            "[30][loss: 3.57][pp:35.41]\n",
            "[40][loss: 3.54][pp:34.33]\n",
            "[50][loss: 3.53][pp:34.01]\n",
            "[60][loss: 3.64][pp:38.28]\n",
            "[70][loss: 3.41][pp:30.15]\n",
            "[80][loss: 3.57][pp:35.60]\n",
            "[90][loss: 3.56][pp:35.03]\n",
            "[100][loss: 3.65][pp:38.46]\n",
            "[110][loss: 3.60][pp:36.69]\n",
            "[120][loss: 3.47][pp:32.19]\n",
            "[130][loss: 3.51][pp:33.34]\n",
            "[140][loss: 3.61][pp:37.05]\n",
            "[150][loss: 3.56][pp:35.08]\n",
            "[160][loss: 3.61][pp:36.94]\n",
            "[170][loss: 3.49][pp:32.91]\n",
            "[180][loss: 3.57][pp:35.42]\n",
            "[190][loss: 3.61][pp:36.84]\n",
            "[200][loss: 3.59][pp:36.09]\n",
            "[210][loss: 3.57][pp:35.36]\n",
            "[220][loss: 3.64][pp:38.13]\n",
            "[230][loss: 3.53][pp:34.13]\n",
            "[240][loss: 3.59][pp:36.11]\n",
            "[250][loss: 3.56][pp:35.30]\n",
            "[260][loss: 3.56][pp:35.19]\n",
            "[270][loss: 3.60][pp:36.75]\n",
            "[280][loss: 3.57][pp:35.40]\n",
            "[290][loss: 3.53][pp:34.01]\n",
            "[300][loss: 3.64][pp:37.96]\n",
            "[310][loss: 3.56][pp:35.15]\n",
            "[320][loss: 3.56][pp:35.29]\n",
            "[330][loss: 3.65][pp:38.64]\n",
            "[340][loss: 3.56][pp:35.06]\n",
            "[350][loss: 3.51][pp:33.29]\n",
            "[360][loss: 3.51][pp:33.53]\n",
            "[370][loss: 3.48][pp:32.51]\n",
            "[380][loss: 3.51][pp:33.33]\n",
            "[390][loss: 3.47][pp:32.28]\n",
            "[400][loss: 3.52][pp:33.80]\n",
            "[410][loss: 3.46][pp:31.97]\n",
            "[420][loss: 3.68][pp:39.60]\n",
            "[430][loss: 3.58][pp:35.85]\n",
            "[440][loss: 3.51][pp:33.33]\n",
            "[450][loss: 3.56][pp:35.15]\n",
            "[460][loss: 3.61][pp:37.07]\n",
            "[470][loss: 3.61][pp:37.01]\n",
            "[480][loss: 3.50][pp:33.02]\n",
            "[490][loss: 3.49][pp:32.63]\n",
            "[500][loss: 3.50][pp:33.02]\n",
            "[510][loss: 3.61][pp:37.12]\n",
            "[520][loss: 3.49][pp:32.93]\n",
            "[530][loss: 3.48][pp:32.61]\n",
            "[540][loss: 3.55][pp:34.95]\n",
            "[550][loss: 3.61][pp:37.08]\n",
            "[560][loss: 3.64][pp:38.18]\n",
            "[570][loss: 3.57][pp:35.67]\n",
            "[580][loss: 3.63][pp:37.71]\n",
            "[590][loss: 3.55][pp:34.95]\n",
            "[600][loss: 3.61][pp:37.05]\n",
            "[610][loss: 3.44][pp:31.04]\n",
            "[620][loss: 3.52][pp:33.69]\n",
            "[630][loss: 3.48][pp:32.34]\n",
            "[640][loss: 3.59][pp:36.34]\n",
            "[650][loss: 3.54][pp:34.49]\n",
            "[660][loss: 3.42][pp:30.59]\n",
            "[670][loss: 3.46][pp:31.85]\n",
            "[680][loss: 3.46][pp:31.77]\n",
            "[690][loss: 3.46][pp:31.87]\n",
            "[700][loss: 3.34][pp:28.26]\n",
            "[710][loss: 3.49][pp:32.78]\n",
            "[720][loss: 3.52][pp:33.80]\n",
            "[730][loss: 3.51][pp:33.46]\n",
            "[740][loss: 3.54][pp:34.51]\n",
            "[750][loss: 3.48][pp:32.53]\n",
            "[760][loss: 3.51][pp:33.60]\n",
            "[770][loss: 3.51][pp:33.57]\n",
            "[780][loss: 3.59][pp:36.27]\n",
            "[790][loss: 3.45][pp:31.40]\n",
            "[800][loss: 3.59][pp:36.25]\n",
            "[810][loss: 3.61][pp:37.08]\n",
            "[820][loss: 3.45][pp:31.55]\n",
            "[830][loss: 3.59][pp:36.28]\n",
            "[840][loss: 3.47][pp:32.28]\n",
            "[850][loss: 3.42][pp:30.69]\n",
            "[860][loss: 3.52][pp:33.82]\n",
            "[870][loss: 3.44][pp:31.15]\n",
            "[880][loss: 3.56][pp:35.32]\n",
            "[890][loss: 3.50][pp:33.28]\n",
            "[900][loss: 3.66][pp:38.71]\n",
            "[Epoch:3] val_loss:3.530 | val_pp:34.12S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Construction workers workers the the of the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players field the field field field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 3.52][pp:33.65]\n",
            "[20][loss: 3.18][pp:23.94]\n",
            "[30][loss: 3.25][pp:25.73]\n",
            "[40][loss: 3.28][pp:26.60]\n",
            "[50][loss: 3.17][pp:23.74]\n",
            "[60][loss: 3.30][pp:27.18]\n",
            "[70][loss: 3.25][pp:25.69]\n",
            "[80][loss: 3.15][pp:23.45]\n",
            "[90][loss: 3.30][pp:27.12]\n",
            "[100][loss: 3.10][pp:22.22]\n",
            "[110][loss: 3.29][pp:26.89]\n",
            "[120][loss: 3.15][pp:23.28]\n",
            "[130][loss: 3.28][pp:26.51]\n",
            "[140][loss: 3.20][pp:24.57]\n",
            "[150][loss: 3.28][pp:26.45]\n",
            "[160][loss: 3.17][pp:23.88]\n",
            "[170][loss: 3.23][pp:25.25]\n",
            "[180][loss: 3.18][pp:24.05]\n",
            "[190][loss: 3.24][pp:25.64]\n",
            "[200][loss: 3.27][pp:26.32]\n",
            "[210][loss: 3.30][pp:27.07]\n",
            "[220][loss: 3.21][pp:24.73]\n",
            "[230][loss: 3.29][pp:26.73]\n",
            "[240][loss: 3.23][pp:25.34]\n",
            "[250][loss: 3.23][pp:25.25]\n",
            "[260][loss: 3.27][pp:26.22]\n",
            "[270][loss: 3.18][pp:23.96]\n",
            "[280][loss: 3.18][pp:23.96]\n",
            "[290][loss: 3.17][pp:23.88]\n",
            "[300][loss: 3.22][pp:25.12]\n",
            "[310][loss: 3.28][pp:26.58]\n",
            "[320][loss: 3.23][pp:25.16]\n",
            "[330][loss: 3.26][pp:26.14]\n",
            "[340][loss: 3.18][pp:24.09]\n",
            "[350][loss: 3.28][pp:26.61]\n",
            "[360][loss: 3.34][pp:28.12]\n",
            "[370][loss: 3.24][pp:25.62]\n",
            "[380][loss: 3.19][pp:24.21]\n",
            "[390][loss: 3.18][pp:24.12]\n",
            "[400][loss: 3.28][pp:26.51]\n",
            "[410][loss: 3.18][pp:24.00]\n",
            "[420][loss: 3.21][pp:24.73]\n",
            "[430][loss: 3.22][pp:24.92]\n",
            "[440][loss: 3.26][pp:26.17]\n",
            "[450][loss: 3.27][pp:26.33]\n",
            "[460][loss: 3.21][pp:24.79]\n",
            "[470][loss: 3.19][pp:24.41]\n",
            "[480][loss: 3.18][pp:24.17]\n",
            "[490][loss: 3.37][pp:29.20]\n",
            "[500][loss: 3.20][pp:24.60]\n",
            "[510][loss: 3.17][pp:23.83]\n",
            "[520][loss: 3.25][pp:25.84]\n",
            "[530][loss: 3.21][pp:24.86]\n",
            "[540][loss: 3.23][pp:25.22]\n",
            "[550][loss: 3.33][pp:27.81]\n",
            "[560][loss: 3.25][pp:25.72]\n",
            "[570][loss: 3.20][pp:24.56]\n",
            "[580][loss: 3.18][pp:23.94]\n",
            "[590][loss: 3.21][pp:24.80]\n",
            "[600][loss: 3.14][pp:23.02]\n",
            "[610][loss: 3.25][pp:25.72]\n",
            "[620][loss: 3.24][pp:25.65]\n",
            "[630][loss: 3.21][pp:24.68]\n",
            "[640][loss: 3.22][pp:24.99]\n",
            "[650][loss: 3.22][pp:25.03]\n",
            "[660][loss: 3.25][pp:25.80]\n",
            "[670][loss: 3.26][pp:26.00]\n",
            "[680][loss: 3.20][pp:24.48]\n",
            "[690][loss: 3.25][pp:25.69]\n",
            "[700][loss: 3.20][pp:24.59]\n",
            "[710][loss: 3.31][pp:27.41]\n",
            "[720][loss: 3.17][pp:23.88]\n",
            "[730][loss: 3.29][pp:26.75]\n",
            "[740][loss: 3.28][pp:26.47]\n",
            "[750][loss: 3.15][pp:23.36]\n",
            "[760][loss: 3.18][pp:24.03]\n",
            "[770][loss: 3.27][pp:26.34]\n",
            "[780][loss: 3.28][pp:26.53]\n",
            "[790][loss: 3.17][pp:23.71]\n",
            "[800][loss: 3.27][pp:26.28]\n",
            "[810][loss: 3.22][pp:25.04]\n",
            "[820][loss: 3.19][pp:24.29]\n",
            "[830][loss: 3.12][pp:22.57]\n",
            "[840][loss: 3.15][pp:23.32]\n",
            "[850][loss: 3.14][pp:23.09]\n",
            "[860][loss: 3.27][pp:26.30]\n",
            "[870][loss: 3.21][pp:24.73]\n",
            "[880][loss: 3.22][pp:25.04]\n",
            "[890][loss: 3.25][pp:25.76]\n",
            "[900][loss: 3.25][pp:25.71]\n",
            "[Epoch:4] val_loss:3.419 | val_pp:30.55S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Workers workers workers to to the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players are on the field field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 3.32][pp:27.75]\n",
            "[20][loss: 3.00][pp:20.06]\n",
            "[30][loss: 2.91][pp:18.43]\n",
            "[40][loss: 2.85][pp:17.36]\n",
            "[50][loss: 2.94][pp:18.99]\n",
            "[60][loss: 2.95][pp:19.02]\n",
            "[70][loss: 2.94][pp:18.99]\n",
            "[80][loss: 2.97][pp:19.40]\n",
            "[90][loss: 2.96][pp:19.30]\n",
            "[100][loss: 2.86][pp:17.47]\n",
            "[110][loss: 2.90][pp:18.09]\n",
            "[120][loss: 2.98][pp:19.65]\n",
            "[130][loss: 2.97][pp:19.52]\n",
            "[140][loss: 2.84][pp:17.08]\n",
            "[150][loss: 2.96][pp:19.29]\n",
            "[160][loss: 2.87][pp:17.65]\n",
            "[170][loss: 2.97][pp:19.42]\n",
            "[180][loss: 2.85][pp:17.26]\n",
            "[190][loss: 2.94][pp:18.94]\n",
            "[200][loss: 2.93][pp:18.79]\n",
            "[210][loss: 2.93][pp:18.70]\n",
            "[220][loss: 2.99][pp:19.86]\n",
            "[230][loss: 2.99][pp:19.85]\n",
            "[240][loss: 2.95][pp:19.14]\n",
            "[250][loss: 2.96][pp:19.22]\n",
            "[260][loss: 2.93][pp:18.76]\n",
            "[270][loss: 3.01][pp:20.21]\n",
            "[280][loss: 3.02][pp:20.41]\n",
            "[290][loss: 3.04][pp:20.88]\n",
            "[300][loss: 2.97][pp:19.42]\n",
            "[310][loss: 2.97][pp:19.55]\n",
            "[320][loss: 3.00][pp:20.00]\n",
            "[330][loss: 3.00][pp:20.15]\n",
            "[340][loss: 2.97][pp:19.55]\n",
            "[350][loss: 3.01][pp:20.25]\n",
            "[360][loss: 2.93][pp:18.69]\n",
            "[370][loss: 2.92][pp:18.48]\n",
            "[380][loss: 3.06][pp:21.29]\n",
            "[390][loss: 2.99][pp:19.81]\n",
            "[400][loss: 2.93][pp:18.64]\n",
            "[410][loss: 2.95][pp:19.05]\n",
            "[420][loss: 2.89][pp:17.95]\n",
            "[430][loss: 2.94][pp:18.98]\n",
            "[440][loss: 2.92][pp:18.51]\n",
            "[450][loss: 2.95][pp:19.05]\n",
            "[460][loss: 3.02][pp:20.57]\n",
            "[470][loss: 3.07][pp:21.56]\n",
            "[480][loss: 2.98][pp:19.75]\n",
            "[490][loss: 2.89][pp:18.01]\n",
            "[500][loss: 3.03][pp:20.68]\n",
            "[510][loss: 2.92][pp:18.59]\n",
            "[520][loss: 2.97][pp:19.48]\n",
            "[530][loss: 2.99][pp:19.96]\n",
            "[540][loss: 2.93][pp:18.81]\n",
            "[550][loss: 2.93][pp:18.65]\n",
            "[560][loss: 2.95][pp:19.06]\n",
            "[570][loss: 2.95][pp:19.08]\n",
            "[580][loss: 2.98][pp:19.73]\n",
            "[590][loss: 2.89][pp:17.94]\n",
            "[600][loss: 3.04][pp:20.84]\n",
            "[610][loss: 3.01][pp:20.29]\n",
            "[620][loss: 3.09][pp:21.97]\n",
            "[630][loss: 3.05][pp:21.20]\n",
            "[640][loss: 2.95][pp:19.19]\n",
            "[650][loss: 2.91][pp:18.32]\n",
            "[660][loss: 2.87][pp:17.68]\n",
            "[670][loss: 3.06][pp:21.23]\n",
            "[680][loss: 2.84][pp:17.08]\n",
            "[690][loss: 2.92][pp:18.47]\n",
            "[700][loss: 3.01][pp:20.34]\n",
            "[710][loss: 3.08][pp:21.66]\n",
            "[720][loss: 3.01][pp:20.29]\n",
            "[730][loss: 3.00][pp:20.07]\n",
            "[740][loss: 3.02][pp:20.54]\n",
            "[750][loss: 2.96][pp:19.38]\n",
            "[760][loss: 2.89][pp:17.92]\n",
            "[770][loss: 2.99][pp:19.95]\n",
            "[780][loss: 3.09][pp:22.02]\n",
            "[790][loss: 3.05][pp:21.22]\n",
            "[800][loss: 2.97][pp:19.48]\n",
            "[810][loss: 2.94][pp:19.00]\n",
            "[820][loss: 3.00][pp:20.00]\n",
            "[830][loss: 3.00][pp:20.13]\n",
            "[840][loss: 3.00][pp:20.10]\n",
            "[850][loss: 2.95][pp:19.05]\n",
            "[860][loss: 3.01][pp:20.25]\n",
            "[870][loss: 2.96][pp:19.28]\n",
            "[880][loss: 3.02][pp:20.49]\n",
            "[890][loss: 3.01][pp:20.22]\n",
            "[900][loss: 3.07][pp:21.51]\n",
            "[Epoch:5] val_loss:3.397 | val_pp:29.88S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Worker worker the the of the . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two football are in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 3.01][pp:20.19]\n",
            "[20][loss: 2.80][pp:16.48]\n",
            "[30][loss: 2.75][pp:15.70]\n",
            "[40][loss: 2.62][pp:13.77]\n",
            "[50][loss: 2.68][pp:14.55]\n",
            "[60][loss: 2.71][pp:14.99]\n",
            "[70][loss: 2.70][pp:14.95]\n",
            "[80][loss: 2.75][pp:15.65]\n",
            "[90][loss: 2.79][pp:16.31]\n",
            "[100][loss: 2.64][pp:14.08]\n",
            "[110][loss: 2.70][pp:14.91]\n",
            "[120][loss: 2.69][pp:14.73]\n",
            "[130][loss: 2.84][pp:17.18]\n",
            "[140][loss: 2.65][pp:14.20]\n",
            "[150][loss: 2.71][pp:15.05]\n",
            "[160][loss: 2.69][pp:14.78]\n",
            "[170][loss: 2.66][pp:14.35]\n",
            "[180][loss: 2.74][pp:15.44]\n",
            "[190][loss: 2.75][pp:15.57]\n",
            "[200][loss: 2.72][pp:15.18]\n",
            "[210][loss: 2.74][pp:15.46]\n",
            "[220][loss: 2.72][pp:15.22]\n",
            "[230][loss: 2.74][pp:15.46]\n",
            "[240][loss: 2.77][pp:16.04]\n",
            "[250][loss: 2.77][pp:15.92]\n",
            "[260][loss: 2.69][pp:14.69]\n",
            "[270][loss: 2.75][pp:15.70]\n",
            "[280][loss: 2.77][pp:15.95]\n",
            "[290][loss: 2.65][pp:14.22]\n",
            "[300][loss: 2.81][pp:16.63]\n",
            "[310][loss: 2.73][pp:15.28]\n",
            "[320][loss: 2.82][pp:16.86]\n",
            "[330][loss: 2.75][pp:15.67]\n",
            "[340][loss: 2.80][pp:16.44]\n",
            "[350][loss: 2.78][pp:16.07]\n",
            "[360][loss: 2.75][pp:15.63]\n",
            "[370][loss: 2.85][pp:17.27]\n",
            "[380][loss: 2.71][pp:14.99]\n",
            "[390][loss: 2.76][pp:15.82]\n",
            "[400][loss: 2.81][pp:16.57]\n",
            "[410][loss: 2.75][pp:15.71]\n",
            "[420][loss: 2.78][pp:16.17]\n",
            "[430][loss: 2.89][pp:18.02]\n",
            "[440][loss: 2.71][pp:15.04]\n",
            "[450][loss: 2.78][pp:16.12]\n",
            "[460][loss: 2.80][pp:16.39]\n",
            "[470][loss: 2.74][pp:15.51]\n",
            "[480][loss: 2.78][pp:16.19]\n",
            "[490][loss: 2.80][pp:16.36]\n",
            "[500][loss: 2.83][pp:17.00]\n",
            "[510][loss: 2.86][pp:17.53]\n",
            "[520][loss: 2.69][pp:14.72]\n",
            "[530][loss: 2.77][pp:15.88]\n",
            "[540][loss: 2.81][pp:16.67]\n",
            "[550][loss: 2.83][pp:16.98]\n",
            "[560][loss: 2.70][pp:14.93]\n",
            "[570][loss: 2.78][pp:16.13]\n",
            "[580][loss: 2.64][pp:14.03]\n",
            "[590][loss: 2.77][pp:16.01]\n",
            "[600][loss: 2.87][pp:17.66]\n",
            "[610][loss: 2.80][pp:16.44]\n",
            "[620][loss: 2.80][pp:16.50]\n",
            "[630][loss: 2.81][pp:16.53]\n",
            "[640][loss: 2.81][pp:16.65]\n",
            "[650][loss: 2.80][pp:16.50]\n",
            "[660][loss: 2.89][pp:17.94]\n",
            "[670][loss: 2.78][pp:16.05]\n",
            "[680][loss: 2.89][pp:18.03]\n",
            "[690][loss: 2.88][pp:17.75]\n",
            "[700][loss: 2.83][pp:16.94]\n",
            "[710][loss: 2.73][pp:15.30]\n",
            "[720][loss: 2.86][pp:17.53]\n",
            "[730][loss: 2.77][pp:15.97]\n",
            "[740][loss: 2.78][pp:16.17]\n",
            "[750][loss: 2.69][pp:14.67]\n",
            "[760][loss: 2.74][pp:15.54]\n",
            "[770][loss: 2.79][pp:16.25]\n",
            "[780][loss: 2.78][pp:16.18]\n",
            "[790][loss: 2.78][pp:16.07]\n",
            "[800][loss: 2.79][pp:16.22]\n",
            "[810][loss: 2.75][pp:15.62]\n",
            "[820][loss: 2.80][pp:16.41]\n",
            "[830][loss: 2.85][pp:17.33]\n",
            "[840][loss: 2.90][pp:18.26]\n",
            "[850][loss: 2.78][pp:16.09]\n",
            "[860][loss: 2.81][pp:16.67]\n",
            "[870][loss: 2.78][pp:16.17]\n",
            "[880][loss: 2.87][pp:17.57]\n",
            "[890][loss: 2.82][pp:16.77]\n",
            "[900][loss: 2.86][pp:17.45]\n",
            "[Epoch:6] val_loss:3.371 | val_pp:29.10S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker worker worker the to the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys soccer soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan running tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a art . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.88][pp:17.90]\n",
            "[20][loss: 2.55][pp:12.78]\n",
            "[30][loss: 2.55][pp:12.85]\n",
            "[40][loss: 2.58][pp:13.20]\n",
            "[50][loss: 2.54][pp:12.62]\n",
            "[60][loss: 2.64][pp:14.02]\n",
            "[70][loss: 2.51][pp:12.36]\n",
            "[80][loss: 2.63][pp:13.84]\n",
            "[90][loss: 2.63][pp:13.89]\n",
            "[100][loss: 2.66][pp:14.30]\n",
            "[110][loss: 2.53][pp:12.61]\n",
            "[120][loss: 2.66][pp:14.23]\n",
            "[130][loss: 2.61][pp:13.61]\n",
            "[140][loss: 2.56][pp:12.96]\n",
            "[150][loss: 2.64][pp:14.00]\n",
            "[160][loss: 2.64][pp:14.07]\n",
            "[170][loss: 2.61][pp:13.56]\n",
            "[180][loss: 2.63][pp:13.88]\n",
            "[190][loss: 2.61][pp:13.55]\n",
            "[200][loss: 2.57][pp:13.01]\n",
            "[210][loss: 2.61][pp:13.55]\n",
            "[220][loss: 2.75][pp:15.68]\n",
            "[230][loss: 2.60][pp:13.44]\n",
            "[240][loss: 2.53][pp:12.58]\n",
            "[250][loss: 2.64][pp:14.02]\n",
            "[260][loss: 2.76][pp:15.82]\n",
            "[270][loss: 2.65][pp:14.14]\n",
            "[280][loss: 2.60][pp:13.48]\n",
            "[290][loss: 2.58][pp:13.19]\n",
            "[300][loss: 2.66][pp:14.24]\n",
            "[310][loss: 2.65][pp:14.14]\n",
            "[320][loss: 2.62][pp:13.74]\n",
            "[330][loss: 2.58][pp:13.20]\n",
            "[340][loss: 2.62][pp:13.67]\n",
            "[350][loss: 2.60][pp:13.47]\n",
            "[360][loss: 2.54][pp:12.74]\n",
            "[370][loss: 2.56][pp:13.00]\n",
            "[380][loss: 2.55][pp:12.87]\n",
            "[390][loss: 2.59][pp:13.38]\n",
            "[400][loss: 2.61][pp:13.65]\n",
            "[410][loss: 2.53][pp:12.55]\n",
            "[420][loss: 2.68][pp:14.54]\n",
            "[430][loss: 2.63][pp:13.93]\n",
            "[440][loss: 2.69][pp:14.66]\n",
            "[450][loss: 2.53][pp:12.61]\n",
            "[460][loss: 2.71][pp:15.10]\n",
            "[470][loss: 2.72][pp:15.20]\n",
            "[480][loss: 2.58][pp:13.19]\n",
            "[490][loss: 2.68][pp:14.53]\n",
            "[500][loss: 2.65][pp:14.09]\n",
            "[510][loss: 2.60][pp:13.52]\n",
            "[520][loss: 2.63][pp:13.86]\n",
            "[530][loss: 2.54][pp:12.71]\n",
            "[540][loss: 2.58][pp:13.14]\n",
            "[550][loss: 2.64][pp:13.98]\n",
            "[560][loss: 2.55][pp:12.85]\n",
            "[570][loss: 2.72][pp:15.21]\n",
            "[580][loss: 2.63][pp:13.94]\n",
            "[590][loss: 2.63][pp:13.87]\n",
            "[600][loss: 2.68][pp:14.64]\n",
            "[610][loss: 2.66][pp:14.33]\n",
            "[620][loss: 2.67][pp:14.50]\n",
            "[630][loss: 2.68][pp:14.61]\n",
            "[640][loss: 2.51][pp:12.34]\n",
            "[650][loss: 2.58][pp:13.17]\n",
            "[660][loss: 2.66][pp:14.36]\n",
            "[670][loss: 2.74][pp:15.50]\n",
            "[680][loss: 2.65][pp:14.17]\n",
            "[690][loss: 2.54][pp:12.67]\n",
            "[700][loss: 2.60][pp:13.53]\n",
            "[710][loss: 2.70][pp:14.83]\n",
            "[720][loss: 2.67][pp:14.45]\n",
            "[730][loss: 2.67][pp:14.40]\n",
            "[740][loss: 2.70][pp:14.92]\n",
            "[750][loss: 2.64][pp:14.04]\n",
            "[760][loss: 2.60][pp:13.52]\n",
            "[770][loss: 2.70][pp:14.93]\n",
            "[780][loss: 2.63][pp:13.91]\n",
            "[790][loss: 2.76][pp:15.73]\n",
            "[800][loss: 2.63][pp:13.94]\n",
            "[810][loss: 2.62][pp:13.78]\n",
            "[820][loss: 2.64][pp:13.99]\n",
            "[830][loss: 2.73][pp:15.26]\n",
            "[840][loss: 2.69][pp:14.79]\n",
            "[850][loss: 2.66][pp:14.34]\n",
            "[860][loss: 2.70][pp:14.91]\n",
            "[870][loss: 2.58][pp:13.20]\n",
            "[880][loss: 2.80][pp:16.52]\n",
            "[890][loss: 2.69][pp:14.67]\n",
            "[900][loss: 2.57][pp:13.04]\n",
            "[Epoch:7] val_loss:3.357 | val_pp:28.69S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker worker worker the the tracks tracks . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.72][pp:15.11]\n",
            "[20][loss: 2.48][pp:11.96]\n",
            "[30][loss: 2.42][pp:11.29]\n",
            "[40][loss: 2.50][pp:12.13]\n",
            "[50][loss: 2.53][pp:12.57]\n",
            "[60][loss: 2.56][pp:12.90]\n",
            "[70][loss: 2.50][pp:12.15]\n",
            "[80][loss: 2.51][pp:12.28]\n",
            "[90][loss: 2.42][pp:11.26]\n",
            "[100][loss: 2.48][pp:11.88]\n",
            "[110][loss: 2.47][pp:11.78]\n",
            "[120][loss: 2.48][pp:11.92]\n",
            "[130][loss: 2.48][pp:11.90]\n",
            "[140][loss: 2.41][pp:11.08]\n",
            "[150][loss: 2.48][pp:11.91]\n",
            "[160][loss: 2.57][pp:13.08]\n",
            "[170][loss: 2.43][pp:11.33]\n",
            "[180][loss: 2.47][pp:11.80]\n",
            "[190][loss: 2.48][pp:11.95]\n",
            "[200][loss: 2.50][pp:12.13]\n",
            "[210][loss: 2.49][pp:12.11]\n",
            "[220][loss: 2.51][pp:12.29]\n",
            "[230][loss: 2.52][pp:12.47]\n",
            "[240][loss: 2.42][pp:11.30]\n",
            "[250][loss: 2.55][pp:12.87]\n",
            "[260][loss: 2.53][pp:12.59]\n",
            "[270][loss: 2.53][pp:12.52]\n",
            "[280][loss: 2.48][pp:12.00]\n",
            "[290][loss: 2.59][pp:13.34]\n",
            "[300][loss: 2.50][pp:12.14]\n",
            "[310][loss: 2.43][pp:11.37]\n",
            "[320][loss: 2.55][pp:12.78]\n",
            "[330][loss: 2.46][pp:11.68]\n",
            "[340][loss: 2.55][pp:12.83]\n",
            "[350][loss: 2.45][pp:11.57]\n",
            "[360][loss: 2.54][pp:12.66]\n",
            "[370][loss: 2.55][pp:12.79]\n",
            "[380][loss: 2.53][pp:12.50]\n",
            "[390][loss: 2.52][pp:12.38]\n",
            "[400][loss: 2.58][pp:13.22]\n",
            "[410][loss: 2.53][pp:12.50]\n",
            "[420][loss: 2.58][pp:13.22]\n",
            "[430][loss: 2.48][pp:11.92]\n",
            "[440][loss: 2.54][pp:12.66]\n",
            "[450][loss: 2.53][pp:12.54]\n",
            "[460][loss: 2.60][pp:13.40]\n",
            "[470][loss: 2.50][pp:12.18]\n",
            "[480][loss: 2.58][pp:13.23]\n",
            "[490][loss: 2.51][pp:12.30]\n",
            "[500][loss: 2.53][pp:12.51]\n",
            "[510][loss: 2.62][pp:13.69]\n",
            "[520][loss: 2.45][pp:11.64]\n",
            "[530][loss: 2.56][pp:12.98]\n",
            "[540][loss: 2.52][pp:12.39]\n",
            "[550][loss: 2.53][pp:12.52]\n",
            "[560][loss: 2.54][pp:12.62]\n",
            "[570][loss: 2.54][pp:12.70]\n",
            "[580][loss: 2.56][pp:12.88]\n",
            "[590][loss: 2.50][pp:12.24]\n",
            "[600][loss: 2.53][pp:12.52]\n",
            "[610][loss: 2.57][pp:13.11]\n",
            "[620][loss: 2.51][pp:12.26]\n",
            "[630][loss: 2.52][pp:12.47]\n",
            "[640][loss: 2.50][pp:12.23]\n",
            "[650][loss: 2.46][pp:11.76]\n",
            "[660][loss: 2.52][pp:12.43]\n",
            "[670][loss: 2.54][pp:12.73]\n",
            "[680][loss: 2.56][pp:12.99]\n",
            "[690][loss: 2.58][pp:13.21]\n",
            "[700][loss: 2.57][pp:13.09]\n",
            "[710][loss: 2.53][pp:12.52]\n",
            "[720][loss: 2.51][pp:12.29]\n",
            "[730][loss: 2.51][pp:12.26]\n",
            "[740][loss: 2.50][pp:12.21]\n",
            "[750][loss: 2.60][pp:13.47]\n",
            "[760][loss: 2.44][pp:11.43]\n",
            "[770][loss: 2.55][pp:12.86]\n",
            "[780][loss: 2.53][pp:12.61]\n",
            "[790][loss: 2.48][pp:11.95]\n",
            "[800][loss: 2.58][pp:13.22]\n",
            "[810][loss: 2.48][pp:11.96]\n",
            "[820][loss: 2.52][pp:12.46]\n",
            "[830][loss: 2.45][pp:11.62]\n",
            "[840][loss: 2.50][pp:12.13]\n",
            "[850][loss: 2.60][pp:13.43]\n",
            "[860][loss: 2.52][pp:12.39]\n",
            "[870][loss: 2.56][pp:12.90]\n",
            "[880][loss: 2.59][pp:13.37]\n",
            "[890][loss: 2.55][pp:12.81]\n",
            "[900][loss: 2.64][pp:14.05]\n",
            "[Epoch:8] val_loss:3.331 | val_pp:27.96S\n",
            "[!] saving model...\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers the the the goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys playing soccer soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan running tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.58][pp:13.19]\n",
            "[20][loss: 2.40][pp:11.08]\n",
            "[30][loss: 2.38][pp:10.85]\n",
            "[40][loss: 2.34][pp:10.41]\n",
            "[50][loss: 2.34][pp:10.40]\n",
            "[60][loss: 2.35][pp:10.47]\n",
            "[70][loss: 2.40][pp:10.98]\n",
            "[80][loss: 2.44][pp:11.42]\n",
            "[90][loss: 2.39][pp:10.96]\n",
            "[100][loss: 2.45][pp:11.63]\n",
            "[110][loss: 2.44][pp:11.50]\n",
            "[120][loss: 2.38][pp:10.82]\n",
            "[130][loss: 2.35][pp:10.46]\n",
            "[140][loss: 2.34][pp:10.43]\n",
            "[150][loss: 2.40][pp:11.01]\n",
            "[160][loss: 2.41][pp:11.17]\n",
            "[170][loss: 2.40][pp:11.02]\n",
            "[180][loss: 2.40][pp:11.07]\n",
            "[190][loss: 2.44][pp:11.47]\n",
            "[200][loss: 2.45][pp:11.54]\n",
            "[210][loss: 2.38][pp:10.75]\n",
            "[220][loss: 2.39][pp:10.89]\n",
            "[230][loss: 2.38][pp:10.85]\n",
            "[240][loss: 2.42][pp:11.20]\n",
            "[250][loss: 2.39][pp:10.96]\n",
            "[260][loss: 2.43][pp:11.31]\n",
            "[270][loss: 2.41][pp:11.10]\n",
            "[280][loss: 2.46][pp:11.70]\n",
            "[290][loss: 2.39][pp:10.96]\n",
            "[300][loss: 2.39][pp:10.92]\n",
            "[310][loss: 2.41][pp:11.16]\n",
            "[320][loss: 2.44][pp:11.51]\n",
            "[330][loss: 2.35][pp:10.52]\n",
            "[340][loss: 2.38][pp:10.82]\n",
            "[350][loss: 2.38][pp:10.83]\n",
            "[360][loss: 2.42][pp:11.23]\n",
            "[370][loss: 2.40][pp:11.06]\n",
            "[380][loss: 2.36][pp:10.59]\n",
            "[390][loss: 2.43][pp:11.37]\n",
            "[400][loss: 2.45][pp:11.57]\n",
            "[410][loss: 2.36][pp:10.64]\n",
            "[420][loss: 2.49][pp:12.11]\n",
            "[430][loss: 2.37][pp:10.65]\n",
            "[440][loss: 2.39][pp:10.91]\n",
            "[450][loss: 2.45][pp:11.62]\n",
            "[460][loss: 2.35][pp:10.51]\n",
            "[470][loss: 2.41][pp:11.19]\n",
            "[480][loss: 2.42][pp:11.20]\n",
            "[490][loss: 2.38][pp:10.79]\n",
            "[500][loss: 2.48][pp:11.97]\n",
            "[510][loss: 2.44][pp:11.46]\n",
            "[520][loss: 2.38][pp:10.85]\n",
            "[530][loss: 2.39][pp:10.93]\n",
            "[540][loss: 2.45][pp:11.62]\n",
            "[550][loss: 2.46][pp:11.70]\n",
            "[560][loss: 2.40][pp:11.05]\n",
            "[570][loss: 2.36][pp:10.61]\n",
            "[580][loss: 2.41][pp:11.19]\n",
            "[590][loss: 2.45][pp:11.57]\n",
            "[600][loss: 2.41][pp:11.19]\n",
            "[610][loss: 2.48][pp:11.97]\n",
            "[620][loss: 2.42][pp:11.27]\n",
            "[630][loss: 2.44][pp:11.42]\n",
            "[640][loss: 2.45][pp:11.61]\n",
            "[650][loss: 2.46][pp:11.67]\n",
            "[660][loss: 2.41][pp:11.16]\n",
            "[670][loss: 2.43][pp:11.38]\n",
            "[680][loss: 2.54][pp:12.68]\n",
            "[690][loss: 2.51][pp:12.25]\n",
            "[700][loss: 2.45][pp:11.56]\n",
            "[710][loss: 2.42][pp:11.29]\n",
            "[720][loss: 2.36][pp:10.56]\n",
            "[730][loss: 2.43][pp:11.30]\n",
            "[740][loss: 2.44][pp:11.51]\n",
            "[750][loss: 2.38][pp:10.79]\n",
            "[760][loss: 2.47][pp:11.85]\n",
            "[770][loss: 2.42][pp:11.30]\n",
            "[780][loss: 2.50][pp:12.15]\n",
            "[790][loss: 2.44][pp:11.48]\n",
            "[800][loss: 2.45][pp:11.60]\n",
            "[810][loss: 2.44][pp:11.45]\n",
            "[820][loss: 2.46][pp:11.67]\n",
            "[830][loss: 2.43][pp:11.37]\n",
            "[840][loss: 2.52][pp:12.38]\n",
            "[850][loss: 2.47][pp:11.78]\n",
            "[860][loss: 2.46][pp:11.75]\n",
            "[870][loss: 2.44][pp:11.43]\n",
            "[880][loss: 2.50][pp:12.20]\n",
            "[890][loss: 2.49][pp:12.02]\n",
            "[900][loss: 2.45][pp:11.59]\n",
            "[Epoch:9] val_loss:3.356 | val_pp:28.66S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers are the goods the goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the the field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.52][pp:12.44]\n",
            "[20][loss: 2.39][pp:10.86]\n",
            "[30][loss: 2.29][pp: 9.89]\n",
            "[40][loss: 2.34][pp:10.41]\n",
            "[50][loss: 2.20][pp: 9.01]\n",
            "[60][loss: 2.20][pp: 9.01]\n",
            "[70][loss: 2.35][pp:10.46]\n",
            "[80][loss: 2.31][pp:10.06]\n",
            "[90][loss: 2.30][pp:10.01]\n",
            "[100][loss: 2.28][pp: 9.75]\n",
            "[110][loss: 2.31][pp:10.05]\n",
            "[120][loss: 2.31][pp:10.10]\n",
            "[130][loss: 2.28][pp: 9.78]\n",
            "[140][loss: 2.33][pp:10.25]\n",
            "[150][loss: 2.27][pp: 9.64]\n",
            "[160][loss: 2.24][pp: 9.37]\n",
            "[170][loss: 2.30][pp: 9.94]\n",
            "[180][loss: 2.29][pp: 9.89]\n",
            "[190][loss: 2.32][pp:10.17]\n",
            "[200][loss: 2.23][pp: 9.27]\n",
            "[210][loss: 2.34][pp:10.39]\n",
            "[220][loss: 2.34][pp:10.33]\n",
            "[230][loss: 2.44][pp:11.45]\n",
            "[240][loss: 2.33][pp:10.30]\n",
            "[250][loss: 2.32][pp:10.14]\n",
            "[260][loss: 2.29][pp: 9.86]\n",
            "[270][loss: 2.35][pp:10.50]\n",
            "[280][loss: 2.39][pp:10.89]\n",
            "[290][loss: 2.39][pp:10.92]\n",
            "[300][loss: 2.32][pp:10.14]\n",
            "[310][loss: 2.29][pp: 9.86]\n",
            "[320][loss: 2.31][pp:10.04]\n",
            "[330][loss: 2.34][pp:10.42]\n",
            "[340][loss: 2.35][pp:10.48]\n",
            "[350][loss: 2.36][pp:10.55]\n",
            "[360][loss: 2.35][pp:10.54]\n",
            "[370][loss: 2.31][pp:10.08]\n",
            "[380][loss: 2.33][pp:10.23]\n",
            "[390][loss: 2.25][pp: 9.52]\n",
            "[400][loss: 2.28][pp: 9.76]\n",
            "[410][loss: 2.33][pp:10.31]\n",
            "[420][loss: 2.36][pp:10.59]\n",
            "[430][loss: 2.36][pp:10.63]\n",
            "[440][loss: 2.36][pp:10.59]\n",
            "[450][loss: 2.31][pp:10.07]\n",
            "[460][loss: 2.33][pp:10.24]\n",
            "[470][loss: 2.30][pp: 9.98]\n",
            "[480][loss: 2.35][pp:10.48]\n",
            "[490][loss: 2.38][pp:10.77]\n",
            "[500][loss: 2.36][pp:10.55]\n",
            "[510][loss: 2.33][pp:10.24]\n",
            "[520][loss: 2.33][pp:10.30]\n",
            "[530][loss: 2.35][pp:10.48]\n",
            "[540][loss: 2.42][pp:11.19]\n",
            "[550][loss: 2.30][pp: 9.94]\n",
            "[560][loss: 2.37][pp:10.65]\n",
            "[570][loss: 2.27][pp: 9.71]\n",
            "[580][loss: 2.34][pp:10.36]\n",
            "[590][loss: 2.41][pp:11.17]\n",
            "[600][loss: 2.40][pp:11.02]\n",
            "[610][loss: 2.37][pp:10.73]\n",
            "[620][loss: 2.32][pp:10.18]\n",
            "[630][loss: 2.30][pp:10.00]\n",
            "[640][loss: 2.40][pp:10.99]\n",
            "[650][loss: 2.39][pp:10.91]\n",
            "[660][loss: 2.35][pp:10.46]\n",
            "[670][loss: 2.41][pp:11.12]\n",
            "[680][loss: 2.33][pp:10.27]\n",
            "[690][loss: 2.35][pp:10.47]\n",
            "[700][loss: 2.34][pp:10.37]\n",
            "[710][loss: 2.30][pp: 9.99]\n",
            "[720][loss: 2.37][pp:10.66]\n",
            "[730][loss: 2.40][pp:11.05]\n",
            "[740][loss: 2.36][pp:10.56]\n",
            "[750][loss: 2.44][pp:11.47]\n",
            "[760][loss: 2.35][pp:10.47]\n",
            "[770][loss: 2.40][pp:10.99]\n",
            "[780][loss: 2.33][pp:10.28]\n",
            "[790][loss: 2.37][pp:10.70]\n",
            "[800][loss: 2.34][pp:10.39]\n",
            "[810][loss: 2.35][pp:10.50]\n",
            "[820][loss: 2.38][pp:10.79]\n",
            "[830][loss: 2.33][pp:10.29]\n",
            "[840][loss: 2.39][pp:10.89]\n",
            "[850][loss: 2.35][pp:10.50]\n",
            "[860][loss: 2.30][pp:10.01]\n",
            "[870][loss: 2.35][pp:10.49]\n",
            "[880][loss: 2.39][pp:10.88]\n",
            "[890][loss: 2.36][pp:10.58]\n",
            "[900][loss: 2.37][pp:10.75]\n",
            "[Epoch:10] val_loss:3.354 | val_pp:28.61S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker worker goods goods goods goods goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys playing soccer soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[TEST] loss: 3.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "79XlHHbCOEeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19621
        },
        "outputId": "79c1e476-619a-4398-b893-ef1e28b16a75"
      },
      "cell_type": "code",
      "source": [
        "for e in range(11, 21):\n",
        "    train(e, seq2seq, optimizer, train_iter, en_size, device, grad_clip, DE, EN)\n",
        "    val_loss = evaluate(seq2seq, val_iter, en_size, device, DE, EN)\n",
        "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, np.exp(val_loss)))\n",
        "\n",
        "    # save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        print(\"[!] saving model...\")\n",
        "        if not os.path.isdir(\"weights\"):\n",
        "            os.makedirs(\"weights\")\n",
        "        torch.save(seq2seq.state_dict(), 'weights/seq2seq_%d.pth' % (e))\n",
        "        best_val_loss = val_loss\n",
        "    \n",
        "    print(\"Samples from test:\")\n",
        "    show_translations(seq2seq, fixed_test_batch, device, trg_sos_token, max_len=20, max_examples=5)\n",
        "    print()\n",
        "        \n",
        "test_loss = evaluate(seq2seq, test_iter, en_size, device, DE, EN)\n",
        "print(\"[TEST] loss:%5.2f\" % test_loss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10][loss: 2.37][pp:10.68]\n",
            "[20][loss: 2.19][pp: 8.96]\n",
            "[30][loss: 2.22][pp: 9.24]\n",
            "[40][loss: 2.18][pp: 8.84]\n",
            "[50][loss: 2.23][pp: 9.34]\n",
            "[60][loss: 2.19][pp: 8.92]\n",
            "[70][loss: 2.25][pp: 9.51]\n",
            "[80][loss: 2.28][pp: 9.82]\n",
            "[90][loss: 2.19][pp: 8.96]\n",
            "[100][loss: 2.29][pp: 9.88]\n",
            "[110][loss: 2.19][pp: 8.89]\n",
            "[120][loss: 2.22][pp: 9.18]\n",
            "[130][loss: 2.23][pp: 9.30]\n",
            "[140][loss: 2.23][pp: 9.30]\n",
            "[150][loss: 2.22][pp: 9.20]\n",
            "[160][loss: 2.25][pp: 9.47]\n",
            "[170][loss: 2.31][pp:10.05]\n",
            "[180][loss: 2.15][pp: 8.58]\n",
            "[190][loss: 2.27][pp: 9.64]\n",
            "[200][loss: 2.17][pp: 8.78]\n",
            "[210][loss: 2.23][pp: 9.26]\n",
            "[220][loss: 2.23][pp: 9.26]\n",
            "[230][loss: 2.21][pp: 9.11]\n",
            "[240][loss: 2.14][pp: 8.51]\n",
            "[250][loss: 2.24][pp: 9.37]\n",
            "[260][loss: 2.28][pp: 9.79]\n",
            "[270][loss: 2.23][pp: 9.33]\n",
            "[280][loss: 2.16][pp: 8.70]\n",
            "[290][loss: 2.20][pp: 9.06]\n",
            "[300][loss: 2.24][pp: 9.44]\n",
            "[310][loss: 2.21][pp: 9.08]\n",
            "[320][loss: 2.25][pp: 9.50]\n",
            "[330][loss: 2.25][pp: 9.50]\n",
            "[340][loss: 2.29][pp: 9.91]\n",
            "[350][loss: 2.21][pp: 9.15]\n",
            "[360][loss: 2.30][pp:10.02]\n",
            "[370][loss: 2.24][pp: 9.36]\n",
            "[380][loss: 2.27][pp: 9.70]\n",
            "[390][loss: 2.26][pp: 9.62]\n",
            "[400][loss: 2.24][pp: 9.42]\n",
            "[410][loss: 2.33][pp:10.25]\n",
            "[420][loss: 2.27][pp: 9.72]\n",
            "[430][loss: 2.33][pp:10.28]\n",
            "[440][loss: 2.30][pp: 9.95]\n",
            "[450][loss: 2.33][pp:10.32]\n",
            "[460][loss: 2.30][pp: 9.97]\n",
            "[470][loss: 2.26][pp: 9.60]\n",
            "[480][loss: 2.18][pp: 8.85]\n",
            "[490][loss: 2.28][pp: 9.82]\n",
            "[500][loss: 2.33][pp:10.27]\n",
            "[510][loss: 2.20][pp: 8.99]\n",
            "[520][loss: 2.28][pp: 9.78]\n",
            "[530][loss: 2.26][pp: 9.55]\n",
            "[540][loss: 2.26][pp: 9.58]\n",
            "[550][loss: 2.22][pp: 9.19]\n",
            "[560][loss: 2.22][pp: 9.22]\n",
            "[570][loss: 2.21][pp: 9.13]\n",
            "[580][loss: 2.18][pp: 8.86]\n",
            "[590][loss: 2.28][pp: 9.74]\n",
            "[600][loss: 2.27][pp: 9.65]\n",
            "[610][loss: 2.24][pp: 9.41]\n",
            "[620][loss: 2.31][pp:10.12]\n",
            "[630][loss: 2.33][pp:10.24]\n",
            "[640][loss: 2.32][pp:10.16]\n",
            "[650][loss: 2.25][pp: 9.48]\n",
            "[660][loss: 2.27][pp: 9.65]\n",
            "[670][loss: 2.31][pp:10.03]\n",
            "[680][loss: 2.34][pp:10.35]\n",
            "[690][loss: 2.24][pp: 9.43]\n",
            "[700][loss: 2.29][pp: 9.85]\n",
            "[710][loss: 2.28][pp: 9.77]\n",
            "[720][loss: 2.33][pp:10.24]\n",
            "[730][loss: 2.26][pp: 9.57]\n",
            "[740][loss: 2.26][pp: 9.57]\n",
            "[750][loss: 2.32][pp:10.15]\n",
            "[760][loss: 2.29][pp: 9.92]\n",
            "[770][loss: 2.27][pp: 9.66]\n",
            "[780][loss: 2.24][pp: 9.35]\n",
            "[790][loss: 2.34][pp:10.38]\n",
            "[800][loss: 2.29][pp: 9.88]\n",
            "[810][loss: 2.21][pp: 9.16]\n",
            "[820][loss: 2.36][pp:10.59]\n",
            "[830][loss: 2.28][pp: 9.82]\n",
            "[840][loss: 2.22][pp: 9.24]\n",
            "[850][loss: 2.32][pp:10.15]\n",
            "[860][loss: 2.29][pp: 9.90]\n",
            "[870][loss: 2.31][pp:10.09]\n",
            "[880][loss: 2.25][pp: 9.45]\n",
            "[890][loss: 2.28][pp: 9.74]\n",
            "[900][loss: 2.19][pp: 8.97]\n",
            "[Epoch:11] val_loss:3.353 | val_pp:28.59S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers goods goods goods goods goods goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys are playing soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.35][pp:10.50]\n",
            "[20][loss: 2.10][pp: 8.14]\n",
            "[30][loss: 2.12][pp: 8.36]\n",
            "[40][loss: 2.11][pp: 8.23]\n",
            "[50][loss: 2.09][pp: 8.06]\n",
            "[60][loss: 2.20][pp: 8.99]\n",
            "[70][loss: 2.18][pp: 8.88]\n",
            "[80][loss: 2.11][pp: 8.22]\n",
            "[90][loss: 2.14][pp: 8.49]\n",
            "[100][loss: 2.23][pp: 9.34]\n",
            "[110][loss: 2.17][pp: 8.72]\n",
            "[120][loss: 2.16][pp: 8.63]\n",
            "[130][loss: 2.10][pp: 8.19]\n",
            "[140][loss: 2.18][pp: 8.82]\n",
            "[150][loss: 2.17][pp: 8.72]\n",
            "[160][loss: 2.08][pp: 8.03]\n",
            "[170][loss: 2.17][pp: 8.73]\n",
            "[180][loss: 2.13][pp: 8.40]\n",
            "[190][loss: 2.20][pp: 9.03]\n",
            "[200][loss: 2.22][pp: 9.23]\n",
            "[210][loss: 2.17][pp: 8.73]\n",
            "[220][loss: 2.19][pp: 8.95]\n",
            "[230][loss: 2.16][pp: 8.66]\n",
            "[240][loss: 2.09][pp: 8.12]\n",
            "[250][loss: 2.21][pp: 9.12]\n",
            "[260][loss: 2.15][pp: 8.63]\n",
            "[270][loss: 2.14][pp: 8.53]\n",
            "[280][loss: 2.17][pp: 8.80]\n",
            "[290][loss: 2.24][pp: 9.42]\n",
            "[300][loss: 2.23][pp: 9.27]\n",
            "[310][loss: 2.22][pp: 9.22]\n",
            "[320][loss: 2.16][pp: 8.67]\n",
            "[330][loss: 2.18][pp: 8.82]\n",
            "[340][loss: 2.18][pp: 8.88]\n",
            "[350][loss: 2.16][pp: 8.63]\n",
            "[360][loss: 2.22][pp: 9.22]\n",
            "[370][loss: 2.17][pp: 8.79]\n",
            "[380][loss: 2.15][pp: 8.61]\n",
            "[390][loss: 2.20][pp: 9.06]\n",
            "[400][loss: 2.20][pp: 8.98]\n",
            "[410][loss: 2.14][pp: 8.47]\n",
            "[420][loss: 2.16][pp: 8.63]\n",
            "[430][loss: 2.15][pp: 8.59]\n",
            "[440][loss: 2.17][pp: 8.78]\n",
            "[450][loss: 2.23][pp: 9.31]\n",
            "[460][loss: 2.14][pp: 8.53]\n",
            "[470][loss: 2.24][pp: 9.40]\n",
            "[480][loss: 2.16][pp: 8.65]\n",
            "[490][loss: 2.23][pp: 9.25]\n",
            "[500][loss: 2.18][pp: 8.82]\n",
            "[510][loss: 2.19][pp: 8.92]\n",
            "[520][loss: 2.17][pp: 8.72]\n",
            "[530][loss: 2.14][pp: 8.54]\n",
            "[540][loss: 2.16][pp: 8.64]\n",
            "[550][loss: 2.23][pp: 9.33]\n",
            "[560][loss: 2.18][pp: 8.83]\n",
            "[570][loss: 2.29][pp: 9.89]\n",
            "[580][loss: 2.13][pp: 8.38]\n",
            "[590][loss: 2.20][pp: 9.06]\n",
            "[600][loss: 2.27][pp: 9.66]\n",
            "[610][loss: 2.20][pp: 9.04]\n",
            "[620][loss: 2.19][pp: 8.91]\n",
            "[630][loss: 2.21][pp: 9.15]\n",
            "[640][loss: 2.24][pp: 9.43]\n",
            "[650][loss: 2.12][pp: 8.34]\n",
            "[660][loss: 2.11][pp: 8.28]\n",
            "[670][loss: 2.21][pp: 9.10]\n",
            "[680][loss: 2.18][pp: 8.89]\n",
            "[690][loss: 2.29][pp: 9.88]\n",
            "[700][loss: 2.18][pp: 8.85]\n",
            "[710][loss: 2.20][pp: 9.06]\n",
            "[720][loss: 2.22][pp: 9.18]\n",
            "[730][loss: 2.22][pp: 9.22]\n",
            "[740][loss: 2.18][pp: 8.83]\n",
            "[750][loss: 2.26][pp: 9.58]\n",
            "[760][loss: 2.20][pp: 9.05]\n",
            "[770][loss: 2.21][pp: 9.08]\n",
            "[780][loss: 2.20][pp: 9.03]\n",
            "[790][loss: 2.15][pp: 8.63]\n",
            "[800][loss: 2.25][pp: 9.52]\n",
            "[810][loss: 2.15][pp: 8.62]\n",
            "[820][loss: 2.24][pp: 9.40]\n",
            "[830][loss: 2.20][pp: 9.05]\n",
            "[840][loss: 2.25][pp: 9.49]\n",
            "[850][loss: 2.20][pp: 9.01]\n",
            "[860][loss: 2.20][pp: 9.05]\n",
            "[870][loss: 2.22][pp: 9.22]\n",
            "[880][loss: 2.20][pp: 9.00]\n",
            "[890][loss: 2.23][pp: 9.28]\n",
            "[900][loss: 2.21][pp: 9.12]\n",
            "[Epoch:12] val_loss:3.360 | val_pp:28.78S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers the goods goods the goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer soccer <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the the field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan running . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.28][pp: 9.75]\n",
            "[20][loss: 2.10][pp: 8.16]\n",
            "[30][loss: 2.08][pp: 8.04]\n",
            "[40][loss: 2.01][pp: 7.49]\n",
            "[50][loss: 2.05][pp: 7.73]\n",
            "[60][loss: 2.03][pp: 7.62]\n",
            "[70][loss: 2.12][pp: 8.35]\n",
            "[80][loss: 2.03][pp: 7.65]\n",
            "[90][loss: 2.10][pp: 8.17]\n",
            "[100][loss: 2.05][pp: 7.74]\n",
            "[110][loss: 2.08][pp: 7.97]\n",
            "[120][loss: 2.10][pp: 8.15]\n",
            "[130][loss: 2.17][pp: 8.80]\n",
            "[140][loss: 2.13][pp: 8.38]\n",
            "[150][loss: 2.06][pp: 7.86]\n",
            "[160][loss: 2.06][pp: 7.87]\n",
            "[170][loss: 2.09][pp: 8.11]\n",
            "[180][loss: 2.10][pp: 8.16]\n",
            "[190][loss: 2.04][pp: 7.70]\n",
            "[200][loss: 2.09][pp: 8.12]\n",
            "[210][loss: 2.13][pp: 8.43]\n",
            "[220][loss: 2.16][pp: 8.68]\n",
            "[230][loss: 2.05][pp: 7.79]\n",
            "[240][loss: 2.03][pp: 7.62]\n",
            "[250][loss: 2.11][pp: 8.27]\n",
            "[260][loss: 2.12][pp: 8.36]\n",
            "[270][loss: 2.10][pp: 8.20]\n",
            "[280][loss: 2.11][pp: 8.29]\n",
            "[290][loss: 2.09][pp: 8.12]\n",
            "[300][loss: 2.16][pp: 8.67]\n",
            "[310][loss: 2.13][pp: 8.40]\n",
            "[320][loss: 2.17][pp: 8.75]\n",
            "[330][loss: 2.04][pp: 7.68]\n",
            "[340][loss: 2.05][pp: 7.75]\n",
            "[350][loss: 2.09][pp: 8.11]\n",
            "[360][loss: 2.20][pp: 9.01]\n",
            "[370][loss: 2.17][pp: 8.79]\n",
            "[380][loss: 2.13][pp: 8.45]\n",
            "[390][loss: 2.07][pp: 7.96]\n",
            "[400][loss: 2.13][pp: 8.40]\n",
            "[410][loss: 2.11][pp: 8.27]\n",
            "[420][loss: 2.12][pp: 8.30]\n",
            "[430][loss: 2.04][pp: 7.72]\n",
            "[440][loss: 2.10][pp: 8.17]\n",
            "[450][loss: 2.15][pp: 8.57]\n",
            "[460][loss: 2.15][pp: 8.55]\n",
            "[470][loss: 2.04][pp: 7.67]\n",
            "[480][loss: 2.08][pp: 8.03]\n",
            "[490][loss: 2.11][pp: 8.26]\n",
            "[500][loss: 2.09][pp: 8.10]\n",
            "[510][loss: 2.18][pp: 8.83]\n",
            "[520][loss: 2.17][pp: 8.75]\n",
            "[530][loss: 2.16][pp: 8.70]\n",
            "[540][loss: 2.09][pp: 8.09]\n",
            "[550][loss: 2.07][pp: 7.94]\n",
            "[560][loss: 2.07][pp: 7.93]\n",
            "[570][loss: 2.13][pp: 8.43]\n",
            "[580][loss: 2.11][pp: 8.24]\n",
            "[590][loss: 2.16][pp: 8.64]\n",
            "[600][loss: 2.08][pp: 8.03]\n",
            "[610][loss: 2.15][pp: 8.61]\n",
            "[620][loss: 2.19][pp: 8.96]\n",
            "[630][loss: 2.20][pp: 9.04]\n",
            "[640][loss: 2.16][pp: 8.63]\n",
            "[650][loss: 2.16][pp: 8.64]\n",
            "[660][loss: 2.14][pp: 8.50]\n",
            "[670][loss: 2.13][pp: 8.41]\n",
            "[680][loss: 2.21][pp: 9.10]\n",
            "[690][loss: 2.17][pp: 8.76]\n",
            "[700][loss: 2.11][pp: 8.23]\n",
            "[710][loss: 2.11][pp: 8.25]\n",
            "[720][loss: 2.16][pp: 8.66]\n",
            "[730][loss: 2.13][pp: 8.38]\n",
            "[740][loss: 2.12][pp: 8.31]\n",
            "[750][loss: 2.09][pp: 8.09]\n",
            "[760][loss: 2.17][pp: 8.79]\n",
            "[770][loss: 2.13][pp: 8.45]\n",
            "[780][loss: 2.14][pp: 8.51]\n",
            "[790][loss: 2.18][pp: 8.83]\n",
            "[800][loss: 2.15][pp: 8.57]\n",
            "[810][loss: 2.13][pp: 8.45]\n",
            "[820][loss: 2.23][pp: 9.26]\n",
            "[830][loss: 2.19][pp: 8.92]\n",
            "[840][loss: 2.15][pp: 8.61]\n",
            "[850][loss: 2.10][pp: 8.19]\n",
            "[860][loss: 2.12][pp: 8.34]\n",
            "[870][loss: 2.21][pp: 9.14]\n",
            "[880][loss: 2.16][pp: 8.67]\n",
            "[890][loss: 2.20][pp: 9.04]\n",
            "[900][loss: 2.12][pp: 8.29]\n",
            "[Epoch:13] val_loss:3.389 | val_pp:29.64S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers the goods goods goods goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys playing playing soccer soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in the the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.16][pp: 8.70]\n",
            "[20][loss: 1.99][pp: 7.35]\n",
            "[30][loss: 2.08][pp: 8.03]\n",
            "[40][loss: 2.05][pp: 7.77]\n",
            "[50][loss: 2.01][pp: 7.44]\n",
            "[60][loss: 1.95][pp: 7.05]\n",
            "[70][loss: 2.06][pp: 7.84]\n",
            "[80][loss: 1.99][pp: 7.33]\n",
            "[90][loss: 2.03][pp: 7.59]\n",
            "[100][loss: 1.99][pp: 7.29]\n",
            "[110][loss: 2.00][pp: 7.40]\n",
            "[120][loss: 2.00][pp: 7.42]\n",
            "[130][loss: 1.95][pp: 7.04]\n",
            "[140][loss: 2.08][pp: 7.98]\n",
            "[150][loss: 2.00][pp: 7.43]\n",
            "[160][loss: 2.00][pp: 7.40]\n",
            "[170][loss: 2.06][pp: 7.85]\n",
            "[180][loss: 2.01][pp: 7.44]\n",
            "[190][loss: 2.04][pp: 7.67]\n",
            "[200][loss: 2.06][pp: 7.83]\n",
            "[210][loss: 2.07][pp: 7.91]\n",
            "[220][loss: 2.01][pp: 7.49]\n",
            "[230][loss: 2.06][pp: 7.82]\n",
            "[240][loss: 2.04][pp: 7.66]\n",
            "[250][loss: 2.03][pp: 7.64]\n",
            "[260][loss: 2.07][pp: 7.90]\n",
            "[270][loss: 2.06][pp: 7.85]\n",
            "[280][loss: 2.08][pp: 8.03]\n",
            "[290][loss: 2.06][pp: 7.83]\n",
            "[300][loss: 2.09][pp: 8.11]\n",
            "[310][loss: 2.03][pp: 7.61]\n",
            "[320][loss: 2.02][pp: 7.56]\n",
            "[330][loss: 1.99][pp: 7.30]\n",
            "[340][loss: 2.00][pp: 7.42]\n",
            "[350][loss: 2.12][pp: 8.33]\n",
            "[360][loss: 2.09][pp: 8.09]\n",
            "[370][loss: 2.04][pp: 7.71]\n",
            "[380][loss: 1.97][pp: 7.19]\n",
            "[390][loss: 2.06][pp: 7.87]\n",
            "[400][loss: 2.00][pp: 7.38]\n",
            "[410][loss: 2.13][pp: 8.38]\n",
            "[420][loss: 2.11][pp: 8.27]\n",
            "[430][loss: 1.99][pp: 7.29]\n",
            "[440][loss: 2.04][pp: 7.73]\n",
            "[450][loss: 2.06][pp: 7.85]\n",
            "[460][loss: 2.03][pp: 7.58]\n",
            "[470][loss: 2.13][pp: 8.41]\n",
            "[480][loss: 2.08][pp: 8.01]\n",
            "[490][loss: 1.99][pp: 7.34]\n",
            "[500][loss: 2.07][pp: 7.95]\n",
            "[510][loss: 2.09][pp: 8.06]\n",
            "[520][loss: 2.01][pp: 7.49]\n",
            "[530][loss: 2.12][pp: 8.35]\n",
            "[540][loss: 2.08][pp: 7.98]\n",
            "[550][loss: 2.06][pp: 7.88]\n",
            "[560][loss: 2.10][pp: 8.14]\n",
            "[570][loss: 2.05][pp: 7.74]\n",
            "[580][loss: 2.11][pp: 8.28]\n",
            "[590][loss: 2.08][pp: 7.98]\n",
            "[600][loss: 2.17][pp: 8.77]\n",
            "[610][loss: 2.06][pp: 7.81]\n",
            "[620][loss: 2.09][pp: 8.09]\n",
            "[630][loss: 2.11][pp: 8.27]\n",
            "[640][loss: 2.07][pp: 7.91]\n",
            "[650][loss: 2.04][pp: 7.66]\n",
            "[660][loss: 2.06][pp: 7.82]\n",
            "[670][loss: 2.11][pp: 8.29]\n",
            "[680][loss: 2.05][pp: 7.73]\n",
            "[690][loss: 2.00][pp: 7.40]\n",
            "[700][loss: 2.12][pp: 8.32]\n",
            "[710][loss: 2.13][pp: 8.40]\n",
            "[720][loss: 2.09][pp: 8.05]\n",
            "[730][loss: 2.08][pp: 8.03]\n",
            "[740][loss: 2.13][pp: 8.43]\n",
            "[750][loss: 2.01][pp: 7.45]\n",
            "[760][loss: 2.03][pp: 7.65]\n",
            "[770][loss: 2.15][pp: 8.61]\n",
            "[780][loss: 2.10][pp: 8.20]\n",
            "[790][loss: 2.07][pp: 7.90]\n",
            "[800][loss: 2.03][pp: 7.59]\n",
            "[810][loss: 2.07][pp: 7.94]\n",
            "[820][loss: 2.05][pp: 7.77]\n",
            "[830][loss: 2.12][pp: 8.33]\n",
            "[840][loss: 2.02][pp: 7.52]\n",
            "[850][loss: 2.03][pp: 7.63]\n",
            "[860][loss: 2.08][pp: 7.99]\n",
            "[870][loss: 2.10][pp: 8.17]\n",
            "[880][loss: 2.20][pp: 9.01]\n",
            "[890][loss: 1.98][pp: 7.27]\n",
            "[900][loss: 2.05][pp: 7.75]\n",
            "[Epoch:14] val_loss:3.407 | val_pp:30.17S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers the goods goods goods goods goods <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing a soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in in the field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.13][pp: 8.44]\n",
            "[20][loss: 1.93][pp: 6.86]\n",
            "[30][loss: 1.89][pp: 6.61]\n",
            "[40][loss: 1.93][pp: 6.92]\n",
            "[50][loss: 1.95][pp: 7.03]\n",
            "[60][loss: 1.96][pp: 7.10]\n",
            "[70][loss: 1.91][pp: 6.79]\n",
            "[80][loss: 1.92][pp: 6.79]\n",
            "[90][loss: 1.95][pp: 7.05]\n",
            "[100][loss: 1.96][pp: 7.12]\n",
            "[110][loss: 2.03][pp: 7.63]\n",
            "[120][loss: 2.03][pp: 7.58]\n",
            "[130][loss: 1.95][pp: 7.02]\n",
            "[140][loss: 2.04][pp: 7.71]\n",
            "[150][loss: 2.06][pp: 7.82]\n",
            "[160][loss: 2.03][pp: 7.62]\n",
            "[170][loss: 1.94][pp: 6.93]\n",
            "[180][loss: 1.98][pp: 7.22]\n",
            "[190][loss: 2.05][pp: 7.77]\n",
            "[200][loss: 1.94][pp: 6.93]\n",
            "[210][loss: 2.00][pp: 7.42]\n",
            "[220][loss: 2.01][pp: 7.49]\n",
            "[230][loss: 1.99][pp: 7.34]\n",
            "[240][loss: 1.97][pp: 7.19]\n",
            "[250][loss: 1.90][pp: 6.66]\n",
            "[260][loss: 1.90][pp: 6.65]\n",
            "[270][loss: 2.03][pp: 7.61]\n",
            "[280][loss: 1.96][pp: 7.10]\n",
            "[290][loss: 1.97][pp: 7.14]\n",
            "[300][loss: 1.93][pp: 6.90]\n",
            "[310][loss: 2.02][pp: 7.57]\n",
            "[320][loss: 1.98][pp: 7.26]\n",
            "[330][loss: 1.93][pp: 6.88]\n",
            "[340][loss: 1.92][pp: 6.84]\n",
            "[350][loss: 1.91][pp: 6.78]\n",
            "[360][loss: 1.92][pp: 6.82]\n",
            "[370][loss: 2.03][pp: 7.60]\n",
            "[380][loss: 2.00][pp: 7.43]\n",
            "[390][loss: 1.96][pp: 7.12]\n",
            "[400][loss: 2.01][pp: 7.47]\n",
            "[410][loss: 2.02][pp: 7.51]\n",
            "[420][loss: 2.01][pp: 7.46]\n",
            "[430][loss: 1.97][pp: 7.18]\n",
            "[440][loss: 2.04][pp: 7.70]\n",
            "[450][loss: 2.01][pp: 7.46]\n",
            "[460][loss: 2.03][pp: 7.63]\n",
            "[470][loss: 2.07][pp: 7.91]\n",
            "[480][loss: 1.95][pp: 7.06]\n",
            "[490][loss: 2.05][pp: 7.78]\n",
            "[500][loss: 1.97][pp: 7.17]\n",
            "[510][loss: 2.03][pp: 7.64]\n",
            "[520][loss: 1.99][pp: 7.28]\n",
            "[530][loss: 1.99][pp: 7.31]\n",
            "[540][loss: 1.95][pp: 7.01]\n",
            "[550][loss: 1.96][pp: 7.10]\n",
            "[560][loss: 2.06][pp: 7.81]\n",
            "[570][loss: 1.99][pp: 7.34]\n",
            "[580][loss: 1.98][pp: 7.23]\n",
            "[590][loss: 1.97][pp: 7.19]\n",
            "[600][loss: 2.04][pp: 7.69]\n",
            "[610][loss: 1.99][pp: 7.34]\n",
            "[620][loss: 2.10][pp: 8.18]\n",
            "[630][loss: 2.06][pp: 7.81]\n",
            "[640][loss: 1.93][pp: 6.89]\n",
            "[650][loss: 2.05][pp: 7.73]\n",
            "[660][loss: 1.95][pp: 7.05]\n",
            "[670][loss: 1.97][pp: 7.16]\n",
            "[680][loss: 2.06][pp: 7.81]\n",
            "[690][loss: 2.05][pp: 7.80]\n",
            "[700][loss: 1.95][pp: 7.00]\n",
            "[710][loss: 2.07][pp: 7.94]\n",
            "[720][loss: 2.04][pp: 7.68]\n",
            "[730][loss: 2.08][pp: 8.01]\n",
            "[740][loss: 2.11][pp: 8.25]\n",
            "[750][loss: 1.99][pp: 7.34]\n",
            "[760][loss: 2.07][pp: 7.93]\n",
            "[770][loss: 2.01][pp: 7.47]\n",
            "[780][loss: 2.02][pp: 7.54]\n",
            "[790][loss: 2.03][pp: 7.61]\n",
            "[800][loss: 1.96][pp: 7.09]\n",
            "[810][loss: 1.98][pp: 7.21]\n",
            "[820][loss: 2.09][pp: 8.06]\n",
            "[830][loss: 2.02][pp: 7.57]\n",
            "[840][loss: 1.99][pp: 7.31]\n",
            "[850][loss: 2.06][pp: 7.88]\n",
            "[860][loss: 2.07][pp: 7.91]\n",
            "[870][loss: 2.01][pp: 7.46]\n",
            "[880][loss: 2.00][pp: 7.36]\n",
            "[890][loss: 1.97][pp: 7.20]\n",
            "[900][loss: 2.02][pp: 7.53]\n",
            "[Epoch:15] val_loss:3.416 | val_pp:30.45S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker worker goods goods goods goods goods goods goods goods <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys are playing soccer . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in in the field <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan running . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.10][pp: 8.16]\n",
            "[20][loss: 1.89][pp: 6.64]\n",
            "[30][loss: 1.91][pp: 6.76]\n",
            "[40][loss: 1.85][pp: 6.35]\n",
            "[50][loss: 1.79][pp: 6.01]\n",
            "[60][loss: 1.95][pp: 7.01]\n",
            "[70][loss: 1.90][pp: 6.68]\n",
            "[80][loss: 1.95][pp: 7.06]\n",
            "[90][loss: 1.90][pp: 6.71]\n",
            "[100][loss: 1.84][pp: 6.27]\n",
            "[110][loss: 1.87][pp: 6.51]\n",
            "[120][loss: 1.96][pp: 7.12]\n",
            "[130][loss: 1.86][pp: 6.42]\n",
            "[140][loss: 1.92][pp: 6.83]\n",
            "[150][loss: 1.93][pp: 6.86]\n",
            "[160][loss: 1.98][pp: 7.26]\n",
            "[170][loss: 1.91][pp: 6.76]\n",
            "[180][loss: 1.82][pp: 6.15]\n",
            "[190][loss: 1.96][pp: 7.13]\n",
            "[200][loss: 1.92][pp: 6.83]\n",
            "[210][loss: 1.88][pp: 6.55]\n",
            "[220][loss: 1.92][pp: 6.84]\n",
            "[230][loss: 1.94][pp: 6.94]\n",
            "[240][loss: 1.86][pp: 6.40]\n",
            "[250][loss: 1.87][pp: 6.50]\n",
            "[260][loss: 2.02][pp: 7.57]\n",
            "[270][loss: 1.93][pp: 6.89]\n",
            "[280][loss: 1.87][pp: 6.50]\n",
            "[290][loss: 1.87][pp: 6.47]\n",
            "[300][loss: 1.89][pp: 6.63]\n",
            "[310][loss: 1.94][pp: 6.93]\n",
            "[320][loss: 1.94][pp: 6.95]\n",
            "[330][loss: 1.90][pp: 6.71]\n",
            "[340][loss: 1.91][pp: 6.73]\n",
            "[350][loss: 1.97][pp: 7.17]\n",
            "[360][loss: 1.98][pp: 7.23]\n",
            "[370][loss: 1.90][pp: 6.72]\n",
            "[380][loss: 1.89][pp: 6.64]\n",
            "[390][loss: 1.97][pp: 7.15]\n",
            "[400][loss: 2.02][pp: 7.57]\n",
            "[410][loss: 1.91][pp: 6.76]\n",
            "[420][loss: 1.87][pp: 6.47]\n",
            "[430][loss: 1.94][pp: 6.96]\n",
            "[440][loss: 1.95][pp: 7.00]\n",
            "[450][loss: 1.90][pp: 6.69]\n",
            "[460][loss: 1.93][pp: 6.88]\n",
            "[470][loss: 1.94][pp: 6.96]\n",
            "[480][loss: 1.96][pp: 7.10]\n",
            "[490][loss: 1.97][pp: 7.20]\n",
            "[500][loss: 1.99][pp: 7.30]\n",
            "[510][loss: 2.01][pp: 7.46]\n",
            "[520][loss: 1.93][pp: 6.88]\n",
            "[530][loss: 1.92][pp: 6.83]\n",
            "[540][loss: 2.02][pp: 7.55]\n",
            "[550][loss: 1.92][pp: 6.81]\n",
            "[560][loss: 1.90][pp: 6.72]\n",
            "[570][loss: 1.93][pp: 6.91]\n",
            "[580][loss: 1.95][pp: 7.06]\n",
            "[590][loss: 1.94][pp: 6.96]\n",
            "[600][loss: 1.90][pp: 6.67]\n",
            "[610][loss: 1.96][pp: 7.09]\n",
            "[620][loss: 1.96][pp: 7.11]\n",
            "[630][loss: 1.97][pp: 7.19]\n",
            "[640][loss: 1.95][pp: 7.04]\n",
            "[650][loss: 1.99][pp: 7.30]\n",
            "[660][loss: 2.04][pp: 7.69]\n",
            "[670][loss: 1.94][pp: 6.93]\n",
            "[680][loss: 2.01][pp: 7.49]\n",
            "[690][loss: 1.97][pp: 7.14]\n",
            "[700][loss: 1.90][pp: 6.67]\n",
            "[710][loss: 1.93][pp: 6.88]\n",
            "[720][loss: 1.97][pp: 7.17]\n",
            "[730][loss: 1.98][pp: 7.27]\n",
            "[740][loss: 2.07][pp: 7.93]\n",
            "[750][loss: 2.01][pp: 7.44]\n",
            "[760][loss: 1.91][pp: 6.75]\n",
            "[770][loss: 1.96][pp: 7.08]\n",
            "[780][loss: 2.01][pp: 7.46]\n",
            "[790][loss: 2.00][pp: 7.41]\n",
            "[800][loss: 1.97][pp: 7.20]\n",
            "[810][loss: 1.89][pp: 6.65]\n",
            "[820][loss: 2.02][pp: 7.53]\n",
            "[830][loss: 2.00][pp: 7.37]\n",
            "[840][loss: 1.97][pp: 7.16]\n",
            "[850][loss: 1.94][pp: 6.97]\n",
            "[860][loss: 1.99][pp: 7.34]\n",
            "[870][loss: 1.93][pp: 6.89]\n",
            "[880][loss: 2.02][pp: 7.51]\n",
            "[890][loss: 1.99][pp: 7.31]\n",
            "[900][loss: 2.06][pp: 7.81]\n",
            "[Epoch:16] val_loss:3.450 | val_pp:31.49S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t worker workers goods goods goods goods goods goods <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys are playing soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players are in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a art . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 2.06][pp: 7.86]\n",
            "[20][loss: 1.87][pp: 6.49]\n",
            "[30][loss: 1.85][pp: 6.37]\n",
            "[40][loss: 1.84][pp: 6.29]\n",
            "[50][loss: 1.83][pp: 6.24]\n",
            "[60][loss: 1.80][pp: 6.06]\n",
            "[70][loss: 1.81][pp: 6.08]\n",
            "[80][loss: 1.82][pp: 6.17]\n",
            "[90][loss: 1.87][pp: 6.49]\n",
            "[100][loss: 1.78][pp: 5.95]\n",
            "[110][loss: 1.90][pp: 6.67]\n",
            "[120][loss: 1.91][pp: 6.73]\n",
            "[130][loss: 1.92][pp: 6.80]\n",
            "[140][loss: 1.80][pp: 6.03]\n",
            "[150][loss: 1.75][pp: 5.75]\n",
            "[160][loss: 1.83][pp: 6.21]\n",
            "[170][loss: 1.84][pp: 6.32]\n",
            "[180][loss: 1.89][pp: 6.59]\n",
            "[190][loss: 1.89][pp: 6.65]\n",
            "[200][loss: 1.87][pp: 6.50]\n",
            "[210][loss: 1.80][pp: 6.08]\n",
            "[220][loss: 1.89][pp: 6.59]\n",
            "[230][loss: 1.96][pp: 7.09]\n",
            "[240][loss: 1.81][pp: 6.09]\n",
            "[250][loss: 1.86][pp: 6.46]\n",
            "[260][loss: 1.88][pp: 6.56]\n",
            "[270][loss: 1.92][pp: 6.81]\n",
            "[280][loss: 1.84][pp: 6.33]\n",
            "[290][loss: 1.85][pp: 6.35]\n",
            "[300][loss: 1.90][pp: 6.66]\n",
            "[310][loss: 1.76][pp: 5.79]\n",
            "[320][loss: 1.90][pp: 6.70]\n",
            "[330][loss: 1.87][pp: 6.47]\n",
            "[340][loss: 1.89][pp: 6.62]\n",
            "[350][loss: 1.86][pp: 6.41]\n",
            "[360][loss: 1.94][pp: 6.98]\n",
            "[370][loss: 1.96][pp: 7.12]\n",
            "[380][loss: 1.84][pp: 6.31]\n",
            "[390][loss: 1.89][pp: 6.64]\n",
            "[400][loss: 1.96][pp: 7.12]\n",
            "[410][loss: 1.92][pp: 6.81]\n",
            "[420][loss: 1.75][pp: 5.75]\n",
            "[430][loss: 1.84][pp: 6.28]\n",
            "[440][loss: 1.87][pp: 6.46]\n",
            "[450][loss: 1.92][pp: 6.79]\n",
            "[460][loss: 1.91][pp: 6.78]\n",
            "[470][loss: 1.91][pp: 6.74]\n",
            "[480][loss: 1.91][pp: 6.76]\n",
            "[490][loss: 1.92][pp: 6.85]\n",
            "[500][loss: 1.88][pp: 6.53]\n",
            "[510][loss: 1.90][pp: 6.68]\n",
            "[520][loss: 1.92][pp: 6.85]\n",
            "[530][loss: 1.89][pp: 6.63]\n",
            "[540][loss: 1.90][pp: 6.71]\n",
            "[550][loss: 1.90][pp: 6.69]\n",
            "[560][loss: 1.84][pp: 6.31]\n",
            "[570][loss: 1.92][pp: 6.80]\n",
            "[580][loss: 1.83][pp: 6.22]\n",
            "[590][loss: 1.95][pp: 7.06]\n",
            "[600][loss: 1.91][pp: 6.75]\n",
            "[610][loss: 1.96][pp: 7.13]\n",
            "[620][loss: 1.88][pp: 6.57]\n",
            "[630][loss: 1.96][pp: 7.13]\n",
            "[640][loss: 1.85][pp: 6.37]\n",
            "[650][loss: 2.06][pp: 7.83]\n",
            "[660][loss: 1.89][pp: 6.65]\n",
            "[670][loss: 1.93][pp: 6.91]\n",
            "[680][loss: 1.91][pp: 6.73]\n",
            "[690][loss: 1.94][pp: 6.94]\n",
            "[700][loss: 1.97][pp: 7.20]\n",
            "[710][loss: 1.95][pp: 7.04]\n",
            "[720][loss: 1.91][pp: 6.72]\n",
            "[730][loss: 1.87][pp: 6.47]\n",
            "[740][loss: 1.92][pp: 6.81]\n",
            "[750][loss: 1.90][pp: 6.65]\n",
            "[760][loss: 1.88][pp: 6.55]\n",
            "[770][loss: 1.90][pp: 6.70]\n",
            "[780][loss: 1.94][pp: 6.93]\n",
            "[790][loss: 1.91][pp: 6.75]\n",
            "[800][loss: 1.99][pp: 7.32]\n",
            "[810][loss: 1.88][pp: 6.54]\n",
            "[820][loss: 1.90][pp: 6.70]\n",
            "[830][loss: 1.92][pp: 6.81]\n",
            "[840][loss: 1.88][pp: 6.56]\n",
            "[850][loss: 1.88][pp: 6.55]\n",
            "[860][loss: 1.88][pp: 6.57]\n",
            "[870][loss: 1.91][pp: 6.77]\n",
            "[880][loss: 1.99][pp: 7.29]\n",
            "[890][loss: 1.90][pp: 6.72]\n",
            "[900][loss: 1.93][pp: 6.86]\n",
            "[Epoch:17] val_loss:3.474 | val_pp:32.28S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Workers workers the goods goods the goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys are playing soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in on the . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 1.98][pp: 7.23]\n",
            "[20][loss: 1.81][pp: 6.12]\n",
            "[30][loss: 1.80][pp: 6.03]\n",
            "[40][loss: 1.78][pp: 5.92]\n",
            "[50][loss: 1.79][pp: 6.00]\n",
            "[60][loss: 1.78][pp: 5.92]\n",
            "[70][loss: 1.80][pp: 6.04]\n",
            "[80][loss: 1.79][pp: 6.01]\n",
            "[90][loss: 1.88][pp: 6.54]\n",
            "[100][loss: 1.78][pp: 5.93]\n",
            "[110][loss: 1.86][pp: 6.39]\n",
            "[120][loss: 1.80][pp: 6.06]\n",
            "[130][loss: 1.88][pp: 6.54]\n",
            "[140][loss: 1.83][pp: 6.26]\n",
            "[150][loss: 1.76][pp: 5.81]\n",
            "[160][loss: 1.79][pp: 6.01]\n",
            "[170][loss: 1.79][pp: 5.98]\n",
            "[180][loss: 1.78][pp: 5.95]\n",
            "[190][loss: 1.74][pp: 5.70]\n",
            "[200][loss: 1.81][pp: 6.09]\n",
            "[210][loss: 1.87][pp: 6.51]\n",
            "[220][loss: 1.76][pp: 5.81]\n",
            "[230][loss: 1.78][pp: 5.95]\n",
            "[240][loss: 1.78][pp: 5.95]\n",
            "[250][loss: 1.88][pp: 6.55]\n",
            "[260][loss: 1.80][pp: 6.05]\n",
            "[270][loss: 1.77][pp: 5.88]\n",
            "[280][loss: 1.84][pp: 6.28]\n",
            "[290][loss: 1.86][pp: 6.45]\n",
            "[300][loss: 1.83][pp: 6.21]\n",
            "[310][loss: 1.80][pp: 6.08]\n",
            "[320][loss: 1.86][pp: 6.45]\n",
            "[330][loss: 1.78][pp: 5.91]\n",
            "[340][loss: 1.81][pp: 6.13]\n",
            "[350][loss: 1.80][pp: 6.06]\n",
            "[360][loss: 1.82][pp: 6.17]\n",
            "[370][loss: 1.89][pp: 6.62]\n",
            "[380][loss: 1.86][pp: 6.39]\n",
            "[390][loss: 1.78][pp: 5.93]\n",
            "[400][loss: 1.88][pp: 6.54]\n",
            "[410][loss: 1.87][pp: 6.50]\n",
            "[420][loss: 1.88][pp: 6.59]\n",
            "[430][loss: 1.85][pp: 6.34]\n",
            "[440][loss: 1.86][pp: 6.41]\n",
            "[450][loss: 1.84][pp: 6.30]\n",
            "[460][loss: 1.84][pp: 6.28]\n",
            "[470][loss: 1.83][pp: 6.26]\n",
            "[480][loss: 1.81][pp: 6.13]\n",
            "[490][loss: 1.83][pp: 6.22]\n",
            "[500][loss: 1.82][pp: 6.15]\n",
            "[510][loss: 1.80][pp: 6.03]\n",
            "[520][loss: 1.88][pp: 6.56]\n",
            "[530][loss: 1.83][pp: 6.25]\n",
            "[540][loss: 1.82][pp: 6.17]\n",
            "[550][loss: 1.85][pp: 6.37]\n",
            "[560][loss: 1.84][pp: 6.32]\n",
            "[570][loss: 1.83][pp: 6.22]\n",
            "[580][loss: 1.79][pp: 5.99]\n",
            "[590][loss: 1.86][pp: 6.40]\n",
            "[600][loss: 1.83][pp: 6.24]\n",
            "[610][loss: 1.86][pp: 6.40]\n",
            "[620][loss: 1.88][pp: 6.56]\n",
            "[630][loss: 1.81][pp: 6.10]\n",
            "[640][loss: 1.86][pp: 6.41]\n",
            "[650][loss: 1.84][pp: 6.28]\n",
            "[660][loss: 1.84][pp: 6.32]\n",
            "[670][loss: 1.84][pp: 6.28]\n",
            "[680][loss: 1.91][pp: 6.73]\n",
            "[690][loss: 1.85][pp: 6.36]\n",
            "[700][loss: 1.85][pp: 6.33]\n",
            "[710][loss: 1.89][pp: 6.63]\n",
            "[720][loss: 1.81][pp: 6.12]\n",
            "[730][loss: 1.92][pp: 6.81]\n",
            "[740][loss: 1.85][pp: 6.37]\n",
            "[750][loss: 1.88][pp: 6.53]\n",
            "[760][loss: 1.81][pp: 6.09]\n",
            "[770][loss: 1.90][pp: 6.66]\n",
            "[780][loss: 1.83][pp: 6.21]\n",
            "[790][loss: 1.92][pp: 6.83]\n",
            "[800][loss: 1.85][pp: 6.34]\n",
            "[810][loss: 1.88][pp: 6.57]\n",
            "[820][loss: 1.90][pp: 6.71]\n",
            "[830][loss: 1.88][pp: 6.54]\n",
            "[840][loss: 1.92][pp: 6.80]\n",
            "[850][loss: 1.84][pp: 6.32]\n",
            "[860][loss: 1.87][pp: 6.51]\n",
            "[870][loss: 1.84][pp: 6.29]\n",
            "[880][loss: 1.87][pp: 6.52]\n",
            "[890][loss: 1.90][pp: 6.69]\n",
            "[900][loss: 1.89][pp: 6.63]\n",
            "[Epoch:18] val_loss:3.496 | val_pp:33.00S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Workers workers goods goods goods goods goods goods <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys playing soccer soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in on the . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a a . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 1.91][pp: 6.77]\n",
            "[20][loss: 1.74][pp: 5.73]\n",
            "[30][loss: 1.70][pp: 5.45]\n",
            "[40][loss: 1.69][pp: 5.40]\n",
            "[50][loss: 1.83][pp: 6.24]\n",
            "[60][loss: 1.75][pp: 5.76]\n",
            "[70][loss: 1.75][pp: 5.78]\n",
            "[80][loss: 1.87][pp: 6.49]\n",
            "[90][loss: 1.76][pp: 5.80]\n",
            "[100][loss: 1.76][pp: 5.78]\n",
            "[110][loss: 1.81][pp: 6.14]\n",
            "[120][loss: 1.76][pp: 5.80]\n",
            "[130][loss: 1.77][pp: 5.89]\n",
            "[140][loss: 1.77][pp: 5.87]\n",
            "[150][loss: 1.81][pp: 6.13]\n",
            "[160][loss: 1.69][pp: 5.44]\n",
            "[170][loss: 1.78][pp: 5.93]\n",
            "[180][loss: 1.77][pp: 5.84]\n",
            "[190][loss: 1.79][pp: 5.97]\n",
            "[200][loss: 1.74][pp: 5.71]\n",
            "[210][loss: 1.80][pp: 6.02]\n",
            "[220][loss: 1.88][pp: 6.53]\n",
            "[230][loss: 1.78][pp: 5.94]\n",
            "[240][loss: 1.79][pp: 5.97]\n",
            "[250][loss: 1.86][pp: 6.41]\n",
            "[260][loss: 1.75][pp: 5.75]\n",
            "[270][loss: 1.77][pp: 5.87]\n",
            "[280][loss: 1.81][pp: 6.08]\n",
            "[290][loss: 1.74][pp: 5.71]\n",
            "[300][loss: 1.76][pp: 5.83]\n",
            "[310][loss: 1.85][pp: 6.35]\n",
            "[320][loss: 1.77][pp: 5.86]\n",
            "[330][loss: 1.79][pp: 5.99]\n",
            "[340][loss: 1.76][pp: 5.79]\n",
            "[350][loss: 1.77][pp: 5.85]\n",
            "[360][loss: 1.81][pp: 6.12]\n",
            "[370][loss: 1.76][pp: 5.81]\n",
            "[380][loss: 1.75][pp: 5.77]\n",
            "[390][loss: 1.76][pp: 5.84]\n",
            "[400][loss: 1.86][pp: 6.41]\n",
            "[410][loss: 1.70][pp: 5.50]\n",
            "[420][loss: 1.67][pp: 5.34]\n",
            "[430][loss: 1.88][pp: 6.55]\n",
            "[440][loss: 1.79][pp: 5.97]\n",
            "[450][loss: 1.75][pp: 5.75]\n",
            "[460][loss: 1.77][pp: 5.88]\n",
            "[470][loss: 1.84][pp: 6.27]\n",
            "[480][loss: 1.86][pp: 6.44]\n",
            "[490][loss: 1.81][pp: 6.09]\n",
            "[500][loss: 1.84][pp: 6.31]\n",
            "[510][loss: 1.76][pp: 5.80]\n",
            "[520][loss: 1.80][pp: 6.03]\n",
            "[530][loss: 1.81][pp: 6.09]\n",
            "[540][loss: 1.79][pp: 5.97]\n",
            "[550][loss: 1.84][pp: 6.29]\n",
            "[560][loss: 1.78][pp: 5.95]\n",
            "[570][loss: 1.79][pp: 5.97]\n",
            "[580][loss: 1.75][pp: 5.73]\n",
            "[590][loss: 1.75][pp: 5.76]\n",
            "[600][loss: 1.70][pp: 5.47]\n",
            "[610][loss: 1.75][pp: 5.77]\n",
            "[620][loss: 1.81][pp: 6.08]\n",
            "[630][loss: 1.83][pp: 6.21]\n",
            "[640][loss: 1.78][pp: 5.95]\n",
            "[650][loss: 1.88][pp: 6.56]\n",
            "[660][loss: 1.67][pp: 5.32]\n",
            "[670][loss: 1.85][pp: 6.38]\n",
            "[680][loss: 1.83][pp: 6.21]\n",
            "[690][loss: 1.90][pp: 6.66]\n",
            "[700][loss: 1.79][pp: 5.98]\n",
            "[710][loss: 1.81][pp: 6.13]\n",
            "[720][loss: 1.77][pp: 5.90]\n",
            "[730][loss: 1.79][pp: 6.00]\n",
            "[740][loss: 1.80][pp: 6.04]\n",
            "[750][loss: 1.78][pp: 5.93]\n",
            "[760][loss: 1.76][pp: 5.81]\n",
            "[770][loss: 1.84][pp: 6.30]\n",
            "[780][loss: 1.77][pp: 5.86]\n",
            "[790][loss: 1.78][pp: 5.92]\n",
            "[800][loss: 1.79][pp: 5.98]\n",
            "[810][loss: 1.76][pp: 5.79]\n",
            "[820][loss: 1.73][pp: 5.64]\n",
            "[830][loss: 1.84][pp: 6.27]\n",
            "[840][loss: 1.82][pp: 6.16]\n",
            "[850][loss: 1.76][pp: 5.81]\n",
            "[860][loss: 1.85][pp: 6.33]\n",
            "[870][loss: 1.89][pp: 6.64]\n",
            "[880][loss: 1.84][pp: 6.30]\n",
            "[890][loss: 1.83][pp: 6.26]\n",
            "[900][loss: 1.84][pp: 6.30]\n",
            "[Epoch:19] val_loss:3.508 | val_pp:33.39S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t workers workers the goods goods goods goods . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two field players in the field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan tan tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a art . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[10][loss: 1.83][pp: 6.21]\n",
            "[20][loss: 1.75][pp: 5.75]\n",
            "[30][loss: 1.67][pp: 5.29]\n",
            "[40][loss: 1.70][pp: 5.47]\n",
            "[50][loss: 1.74][pp: 5.70]\n",
            "[60][loss: 1.68][pp: 5.34]\n",
            "[70][loss: 1.69][pp: 5.39]\n",
            "[80][loss: 1.74][pp: 5.68]\n",
            "[90][loss: 1.77][pp: 5.87]\n",
            "[100][loss: 1.64][pp: 5.15]\n",
            "[110][loss: 1.70][pp: 5.47]\n",
            "[120][loss: 1.73][pp: 5.61]\n",
            "[130][loss: 1.64][pp: 5.13]\n",
            "[140][loss: 1.77][pp: 5.88]\n",
            "[150][loss: 1.77][pp: 5.86]\n",
            "[160][loss: 1.67][pp: 5.30]\n",
            "[170][loss: 1.76][pp: 5.81]\n",
            "[180][loss: 1.76][pp: 5.81]\n",
            "[190][loss: 1.60][pp: 4.97]\n",
            "[200][loss: 1.79][pp: 5.99]\n",
            "[210][loss: 1.75][pp: 5.75]\n",
            "[220][loss: 1.65][pp: 5.22]\n",
            "[230][loss: 1.74][pp: 5.70]\n",
            "[240][loss: 1.80][pp: 6.07]\n",
            "[250][loss: 1.66][pp: 5.27]\n",
            "[260][loss: 1.73][pp: 5.62]\n",
            "[270][loss: 1.79][pp: 5.98]\n",
            "[280][loss: 1.77][pp: 5.89]\n",
            "[290][loss: 1.76][pp: 5.79]\n",
            "[300][loss: 1.76][pp: 5.81]\n",
            "[310][loss: 1.69][pp: 5.41]\n",
            "[320][loss: 1.70][pp: 5.48]\n",
            "[330][loss: 1.75][pp: 5.73]\n",
            "[340][loss: 1.72][pp: 5.61]\n",
            "[350][loss: 1.74][pp: 5.69]\n",
            "[360][loss: 1.76][pp: 5.79]\n",
            "[370][loss: 1.79][pp: 5.97]\n",
            "[380][loss: 1.78][pp: 5.95]\n",
            "[390][loss: 1.72][pp: 5.60]\n",
            "[400][loss: 1.77][pp: 5.88]\n",
            "[410][loss: 1.66][pp: 5.27]\n",
            "[420][loss: 1.74][pp: 5.67]\n",
            "[430][loss: 1.75][pp: 5.76]\n",
            "[440][loss: 1.79][pp: 6.00]\n",
            "[450][loss: 1.70][pp: 5.46]\n",
            "[460][loss: 1.73][pp: 5.61]\n",
            "[470][loss: 1.75][pp: 5.74]\n",
            "[480][loss: 1.72][pp: 5.61]\n",
            "[490][loss: 1.75][pp: 5.75]\n",
            "[500][loss: 1.79][pp: 6.01]\n",
            "[510][loss: 1.70][pp: 5.46]\n",
            "[520][loss: 1.75][pp: 5.76]\n",
            "[530][loss: 1.73][pp: 5.63]\n",
            "[540][loss: 1.72][pp: 5.57]\n",
            "[550][loss: 1.73][pp: 5.64]\n",
            "[560][loss: 1.78][pp: 5.91]\n",
            "[570][loss: 1.71][pp: 5.55]\n",
            "[580][loss: 1.71][pp: 5.52]\n",
            "[590][loss: 1.73][pp: 5.63]\n",
            "[600][loss: 1.78][pp: 5.94]\n",
            "[610][loss: 1.76][pp: 5.82]\n",
            "[620][loss: 1.72][pp: 5.57]\n",
            "[630][loss: 1.70][pp: 5.49]\n",
            "[640][loss: 1.76][pp: 5.79]\n",
            "[650][loss: 1.76][pp: 5.81]\n",
            "[660][loss: 1.78][pp: 5.90]\n",
            "[670][loss: 1.70][pp: 5.50]\n",
            "[680][loss: 1.76][pp: 5.82]\n",
            "[690][loss: 1.79][pp: 6.01]\n",
            "[700][loss: 1.73][pp: 5.66]\n",
            "[710][loss: 1.81][pp: 6.11]\n",
            "[720][loss: 1.78][pp: 5.93]\n",
            "[730][loss: 1.80][pp: 6.04]\n",
            "[740][loss: 1.74][pp: 5.70]\n",
            "[750][loss: 1.79][pp: 5.98]\n",
            "[760][loss: 1.75][pp: 5.76]\n",
            "[770][loss: 1.80][pp: 6.07]\n",
            "[780][loss: 1.71][pp: 5.55]\n",
            "[790][loss: 1.76][pp: 5.79]\n",
            "[800][loss: 1.75][pp: 5.75]\n",
            "[810][loss: 1.77][pp: 5.89]\n",
            "[820][loss: 1.77][pp: 5.89]\n",
            "[830][loss: 1.79][pp: 5.97]\n",
            "[840][loss: 1.80][pp: 6.03]\n",
            "[850][loss: 1.73][pp: 5.64]\n",
            "[860][loss: 1.86][pp: 6.42]\n",
            "[870][loss: 1.76][pp: 5.81]\n",
            "[880][loss: 1.80][pp: 6.02]\n",
            "[890][loss: 1.79][pp: 5.97]\n",
            "[900][loss: 1.82][pp: 6.18]\n",
            "[Epoch:20] val_loss:3.531 | val_pp:34.15S\n",
            "Samples from test:\n",
            "input:\t Arbeiter diskutieren neben den Schienen . <eos>\n",
            "pred:\t Workers workers goods goods goods goods goods goods <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Construction workers having a discussion by the tracks . <eos>\n",
            "\n",
            "input:\t Zwei Jungen spielen gegeneinander Fußball . <eos>\n",
            "pred:\t Two boys boys playing soccer soccer . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two boys play soccer against each other . <eos>\n",
            "\n",
            "input:\t Zwei Fußballmannschaften auf dem Feld . <eos>\n",
            "pred:\t Two players players in on field . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t Two soccer teams are on the field . <eos>\n",
            "\n",
            "input:\t Ein hellbrauner Hund läuft bergauf . <eos>\n",
            "pred:\t A tan tan tan running tan <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t A light brown dog is running up . <eos>\n",
            "\n",
            "input:\t Leute bewundern ein Kunstwerk . <eos>\n",
            "pred:\t People are a a art . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "gt:\t People are admiring a work of art . <eos>\n",
            "\n",
            "\n",
            "[TEST] loss: 3.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_h91LhEzPdx-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yeah, we did it!"
      ]
    }
  ]
}